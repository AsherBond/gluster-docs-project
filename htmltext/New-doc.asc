FIXME: *work in progress - suggestions welcome*

Contents
~~~~~~~~

* link:#Introduction[1 Introduction]
** link:#What_is_GlusterFS[1.1 What is GlusterFS]
** link:#Why_we_wrote_another_filesystem[1.2 Why we wrote another
filesystem]
** link:#Where_is_white_paper_about_your_filesystem_format[1.3 Where is
white paper about your filesystem format]
** link:#Why_use_GlusterFS[1.4 Why use GlusterFS]

Introduction
------------

What is GlusterFS
~~~~~~~~~~~~~~~~~

A largely scalable clustered filesystem in userspace (FUSE). GlusterFS
is highly adaptable, feature rich, POSIX compliant and has a layered
design. GlusterFS can work on any type of interconnect, IB or gig/e, or
10gig/e.

Why we wrote another filesystem
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

We ourself wanted to deploy a large filesystem for one of our customer,
who needed Petabyte + storage. For that type of large volume, we needed
a filesystem which addresses reliability, maintainability (ease of use),
and scalability. We couldn't find all these three in a single
filesystem. This is the reason why we started our own filesystem.

Where is white paper about your filesystem format
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

GlusterFS is a clustered network filesystem, not a disk based
filesystem. with GlusterFS, on back end one can keep their own pet
filesystem in the back end directory. GlusterFS doesn't have any format
of its own to keep data. A back end of GlusterFS can be visualized same
as NFS export point, where you can keep the directories as it is. As its
not a new filesystem format by itself, we happen to spend more time
coding than publishing a white paper. But we will come out with one soon
until then the answer will be _"No, we don't have one yet!"_.

Why use GlusterFS
~~~~~~~~~~~~~~~~~

As told earlier, we wrote it to address three major issues seen in large
scale storage solutions.

1.  Scalability
2.  Performance
3.  Manageability

The Userspace design approach which we took helped us to achieve these
goals faster, in a much efficient way.

Its well known that filesystems are core part of the OS, hence has to be
in kernel space. But when it comes to network filesystems, this is is
not true. The delay which is caused by network latency is much larger
than context switch overhead caused by being in userspace. And if the
network latency is reduced by using Infiniband or 10gigabyte cards, one
can do RDMA from userspace to remote machines, hence the question of
context switch doesn't arise. So, the question of performance doesn't
come into picture.

Also as we are in userspace, the development cycles for any feature is
quite less compared to kernel based filesystem. Any problem with the
filesystem, just a application restart is enough (say a umount/mount is
enough). No need of reboots, no nightmares of kernel panics.
