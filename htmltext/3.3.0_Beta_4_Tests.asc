http://www.gluster.org/community/documentation/index.php/GlusterFest[Back
to GlusterFest]

Contents
~~~~~~~~

* link:#Welcome[1 Welcome]
* link:#Assemble_test_machines[2 Assemble test machines]
* link:#Bugs.2C_Bugzilla[3 Bugs, Bugzilla]
* link:#Build_Gluster[4 Build Gluster]
* link:#Set_up_test_machines[5 Set up test machines]
* #Test_1_.E2.80.94_Assemble_a_cluster:[6 Test 1 — Assemble a cluster:]
* #Test_2_.E2.80.94_Create_volumes:[7 Test 2 — Create volumes:]
* #Test_3_.E2.80.94_Start_all_volumes:[8 Test 3 — Start all volumes:]
*
#Test_4a_.E2.80.94_Mount_all_volumes_on_client.28s.29_using_native_gluster:[9
Test 4a — Mount all volumes on client(s) using native gluster:]
* #Test_4b_.E2.80.94_Mount_all_volumes_on_client.28s.29_using_NFS:[10
Test 4b — Mount all volumes on client(s) using NFS:]
* #Test_5a_.E2.80.94_Run_I.2FO_on_client.28s.29_over_native_gluster:[11
Test 5a — Run I/O on client(s) over native gluster:]
* #Test_5b_.E2.80.94_Run_I.2FO_on_client.28s.29_over_NFS:[12 Test 5b —
Run I/O on client(s) over NFS:]
* #Test_6_.E2.80.94_Stop_all_volumes:[13 Test 6 — Stop all volumes:]
* #Test_7_.E2.80.94_Delete_all_volumes:[14 Test 7 — Delete all volumes:]
* #Test_8_.E2.80.94_Detach_all_peers:[15 Test 8 — Detach all peers:]

Welcome
^^^^^^^

Thank you for helping to test the newest release of GlusterFS.

Assemble test machines
^^^^^^^^^^^^^^^^^^^^^^

These tests use one, two, four, and optionally more servers; and one or
more clients. Use qemu/kvm or other virtualization products to create as
many guests as you need for the servers and clients. Of course you may
also use bare metal if you have it available. Setting up your servers
and clients is left as an exercise. Remember to enter the hostnames in
DNS and/or in /etc/hosts on all the nodes.

Bugs, Bugzilla
^^^^^^^^^^^^^^

Any crashes, anomalies, please file a BZ at
https://bugzilla.redhat.com/enter_bug.cgi?product=GlusterFS[https://bugzilla.redhat.com/enter_bug.cgi?product=GlusterFS]

Note that you will need to create an account if you don't already have
one.

Build Gluster
^^^^^^^^^^^^^

Get the source from
http://bits.gluster.com/pub/gluster/glusterfs/src/glusterfs-3.3.0beta4.tar.gz[http://bits.gluster.com/pub/gluster/glusterfs/src/glusterfs-3.3.0beta4.tar.gz]

Or get pre-built binaries from
http://bits.gluster.com/pub/gluster/glusterfs/3.3.0beta4/x86_64/[http://bits.gluster.com/pub/gluster/glusterfs/3.3.0beta4/x86_64/]

To build from source there are a few prerequisites (Fedora):

* automake
* autoconf
* libxml2-devel
* ncurses-devel
* readline-devel
* openssl-devel

In Ubuntu/Debian:

* automake
* autoconf
* libxml2-dev
* libncurses-dev
* libreadline-dev
* libssl-dev

You may also need these:

* libibverbs-devel
* libtool
* python-ctypes (Fedora)
* python-ctypeslib (Ubuntu/Debian)

In addition to the usual gcc, make, bison, and flex.

N.B. all the following commands should be run as root or with sudo.

Unpack the source, configure, and build:
` % tar xzpf glusterfs-3.3.0beta4.tar.gz % cd glusterfs-3.3.0beta4 % ./autogen.sh % ./configure % make`

Set up test machines
^^^^^^^^^^^^^^^^^^^^

To install glusterfs:
` % make install <repeat on each server and client>`

Or install the prebuilt binaries referenced above.

Create brick directories:
` % mkdir /tmp/test{1,2,3,4,5} % chmod 01777 /tmp/test? <repeat on each server>`

Start glusterd: ` % /usr/local/sbin/glusterd <repeat on each server>`

On each server use your prefered method to open ports 24007-24030 and
38465-38468 in your firewall. (Or completely disable your firewall.)

Test 1 — Assemble a cluster:
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

 +
<On one server node (e.g. <node1name>)
` % /usr/local/sbin/gluster peer status Number of Peers: 0 % /usr/local/sbin/gluster peer probe <node2name> Probe successful % /usr/local/sbin/gluster peer probe <node3name> Probe successful % /usr/local/sbin/gluster peer probe <node4name> Probe successful % /usr/local/sbin/gluster peer status Number of Peers: 3  Hostname: <node2name> Uuid: 5d25363f-1d09-44b5-94eb-258be64ec3f3 State: Peer in Cluster (Connected)  Hostname: <node3name> Uuid: 6e989fab-3126-43a2-9946-9bab2b90775d State: Peer in Cluster (Connected)  Hostname: <node4name> Uuid: 0d4aedc5-239b-49b2-94e1-bb1eb9d21820 State: Peer in Cluster (Connected)`

Verify that each node has a unique UUID and all are Connected

Test 2 — Create volumes:
^^^^^^^^^^^^^^^^^^^^^^^^

 +
<On one of the server nodes create a single brick volume>
` % /usr/local/sbin/gluster volume create test1 <node1name>:/tmp/test1 Creation of volume test1 has been successful. Please start the volume to access data.`
 +
  +
<On one of the server nodes create a two brick distributed (DHT) volume>
 +
% /usr/local/sbin/gluster volume create test2 <node1name>:/tmp/test2
<node2name>:/tmp/test2  +
Creation of volume test2 has been successful. Please start the volume to
access data. </code>  +
  +
<On one of the server nodes create a two brick replicated (AFR) volume>
 +
% /usr/local/sbin/gluster volume create test3 replica 2
<node1name>:/tmp/test3 <node2name>:/tmp/test3  +
Creation of volume test3 has been successful. Please start the volume to
access data. </code>  +
  +
<On one of the server nodes create a four brick distributed, replicated
volume>  +
% /usr/local/sbin/gluster volume create test4 replica 2
<node1name>:/tmp/test4 <node2name>:/tmp/test4 <node3name>:/tmp/test4
<node4name>:/tmp/test4  +
Creation of volume test4 has been successful. Please start the volume to
access data. </code>  +
  +
<On one of the server nodes create a two brick striped volume>
` % /usr/local/sbin/gluster volume create test5 stripe 2 <node1name>:/tmp/test5 <node2name>:/tmp/test5 Creation of volume test5 has been successful. Please start the volume to access data.  <On one of the server nodes query volume info> % gluster volume info  Volume Name: test1 Type: Distribute Status: Started Number of Bricks: 1 Transport-type: tcp Bricks:  Brick1: <node1name>:/tmp/test1  Volume Name: test2 Type: Distribute Status: Created Number of Bricks: 2 Transport-type: tcp Bricks: Brick1: <node1name>:/tmp/test2 Brick2: <node2name>:/tmp/test2  Volume Name: test3 Type: Replicate Status: Created Number of Bricks: 2 Transport-type: tcp Bricks: Brick1: <node1name>:/tmp/test3 Brick2: <node2name>:/tmp/test3  Volume Name: test4 Type: Distributed-Replicate Status: Created Number of Bricks: 2 x 2 = 4 Transport-type: tcp Bricks: Brick1: <node1name>:/tmp/test4 Brick2: <node2name>:/tmp/test4 Brick1: <node3name>:/tmp/test4 Brick2: <node4name>:/tmp/test4  Volume Name: test5 Type: Stripe Status: Created Number of Bricks: 2 Transport-type: tcp Bricks: Brick1: <node1name>:/tmp/test5 Brick2: <node2name>:/tmp/test5`

Verify that each volume is correctly created.

Test 3 — Start all volumes:
^^^^^^^^^^^^^^^^^^^^^^^^^^^

 +
<On one of the server nodes start the volumes>
` % /usr/local/sbin/gluster volume start test1 Starting volume test1 has been successful  % /usr/local/sbin/gluster volume start test2 Starting volume test2 has been successful  % /usr/local/sbin/gluster volume start test3 Starting volume test3 has been successful  % /usr/local/sbin/gluster volume start test4 Starting volume test4 has been successful  % /usr/local/sbin/gluster volume start test5 Starting volume test5 has been successful  <On one of the server nodes query volume info> % gluster volume info   Volume Name: test1 Type: Distribute Status: Started Number of Bricks: 1 Transport-type: tcp Bricks:  Brick1: <node1name>:/tmp/test1  Volume Name: test2 Type: Distribute Status: Started Number of Bricks: 2 Transport-type: tcp Bricks: Brick1: <node1name>:/tmp/test2 Brick2: <node2name>:/tmp/test2  Volume Name: test3 Type: Replicate Status: Started Number of Bricks: 2 Transport-type: tcp Bricks: Brick1: <node1name>:/tmp/test3 Brick2: <node2name>:/tmp/test3  Volume Name: test4 Type: Distributed-Replicate Status: Started Number of Bricks: 2 x 2 = 4 Transport-type: tcp Bricks: Brick1: <node1name>:/tmp/test4 Brick2: <node2name>:/tmp/test4 Brick1: <node3name>:/tmp/test4 Brick2: <node4name>:/tmp/test4  Volume Name: test5 Type: Stripe Status: Started Number of Bricks: 2 Transport-type: tcp Bricks: Brick1: <node1name>:/tmp/test5 Brick2: <node2name>:/tmp/test5`

Verify that each volume is correctly started.

Test 4a — Mount all volumes on client(s) using native gluster:
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

 +
<On one or more of the client nodes mount the volumes>
` % mount -t glusterfs <node1name>:test1 <mountpoint1>  % mount -t glusterfs <node1name>:test2 <mountpoint2>  % mount -t glusterfs <node1name>:test3 <mountpoint3>  % mount -t glusterfs <node1name>:test4 <mountpoint4>  % mount -t glusterfs <node1name>:test5 <mountpoint5>`

Verify using mount or df that all volumes have been successfully mounted

Test 4b — Mount all volumes on client(s) using NFS:
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

 +
<On one or more of the client nodes mount the volumes>
` % mount <node1name>:/test1 <mountpoint1>  % mount <node1name>:/test2 <mountpoint2>  % mount <node1name>:/test3 <mountpoint3>  % mount <node1name>:/test4 <mountpoint4>  % mount <node1name>:/test5 <mountpoint5>`

Verify using mount or df that all volumes have been successfully mounted

Test 5a — Run I/O on client(s) over native gluster:
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Drive I/O to the volumes from the clients. E.g. this could be as simple
as copying a few files to and from the volume or as involved as running
your favorite benchmark. Try to exercise as many different forms of file
I/O as you can. Light load, heavy load, be creative.

Test 5b — Run I/O on client(s) over NFS:
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

See test 5a.

Test 6 — Stop all volumes:
^^^^^^^^^^^^^^^^^^^^^^^^^^

TBD

Test 7 — Delete all volumes:
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

TBD

Test 8 — Detach all peers:
^^^^^^^^^^^^^^^^^^^^^^^^^^

TBD
