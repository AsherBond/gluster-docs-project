Glusterfs architectural overview and hackers guide.

Intended audience:

----------------------------------------------
- people that want to read / modify the source
- advanced users
----------------------------------------------

Here you'll find a roadmap to the currently stable version of the
glusterfs codebase, a quick overview of how it all ties toghether and
what bit of code lives where and an overview of the library as well as
the main internal datastructures that the server and the client use.

As of this writing glusterfs stable is at version 1.3.1, I'll try to
keep this document up to date as newer versions are being created.

The glusterfs clustering filesystem consists of three main components,
the library, the server and the client.

Glusterfs is a very modular program, pieces of the binaries are loaded
(or not) at runtime as required. This makes for a fairly simple main
line and allows for extending the program in a relatively
straightforward way.

The library contains common code for both the daemon and the client.

The server (glusterfsd) is the part where the actual data gets stored.
The glusterfsd program exposes either an ethernet or an infiniband
connection to the outside world where one or more clients can connect.

The client exposes one or more bricks that it has connected to through
ethernet or infiniband via the 'fuse' kernel module to applications
running on the same machine as the client. The fuse module alows the
user mode program to be mounted just like any other file system.

Different modules in the client can be added in pretty much any order
stacked on top of each other as though they are layers in a cake. Each
layer is in glusterfs parlance called a 'translator'. A nice example of
how this modularity allows for innovative solutions is displayed in the
'debug translator'.

Translators are grouped by function, at this moment there are several
groups:

------------------------------
- protocol
- performance
- clustering
- 'features' (such as locking)
- encryption
------------------------------

More translator groups and instances of those groups can be added
without ripping up the rest of the code.

In Unify mode (unify exposes a series of underlying storage 'bricks' as
a single filesystem) there is currently one more visible element in the
glusterfs system that deserves a mention here, which is the 'namespace'.
The main function (besides some smaller ones) of the namespace is to
make sure that inode numbers are consistent across all the nodes in the
unify group. The intention of the glusterfs team is that the namespace
will disappear and that glusterfs will be a true 'shared nothing'
cluster filesystem.

Source code, top levels:

` `

------------------------------------------------------------------------------------------------------------------------
./glusterfs-fuse/        client code and interface with the fuse kernel module
./libglusterfs/          library code, common to all glusterfs related programs
./xlators/               all the translators grouped by function
./xlators/cluster/       clustering translaters (unify, afr, stripe)
./xlators/storage/       storage translators (posix)
./xlators/protocol/      protocol translators (client side, server side)
./xlators/performance/   performance translators (write-behind, read-ahaed, caching, iothreads)
./xlators/debug/         debug translators (trace)
./xlators/features/      'feature' translators
./xlators/encryption/    encryption translators (currently only a 'demo' rot13 translator)
./scheduler/             contains the various schedulers that can be used in combination with the clustering translators
./transport/             transport layers (tcp, infiniband)
./glusterfsd/            server code
------------------------------------------------------------------------------------------------------------------------

'''''

todo:

Library functions

Client datastructures

Server datastructures
