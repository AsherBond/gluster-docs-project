Contents
~~~~~~~~

* link:#Key_Differences[1 Key Differences]
* link:#Proposed_Near-term_Glusterfs_Enhancements[2 Proposed Near-term
Glusterfs Enhancements]
* link:#Proposed_Intermediate-term_Glusterfs_Enhancements[3 Proposed
Intermediate-term Glusterfs Enhancements]
* link:#Other_Potential_Glusterfs_Enhancements_For_Consideration[4 Other
Potential Glusterfs Enhancements For Consideration]

Key Differences
---------------

Proposed Near-term Glusterfs Enhancements
-----------------------------------------

* Striping - Should work, as-is
** Provide best practices for Striping use
** Explore cascading of DHT and Stripe, where multipler stripe sets in
same volume share same local filesystems
*** Example: (DHT (Stripe Brick1a Brick2a Brick3a) (Stripe Brick3b
Brick2b Brick1b))

* AFR Path Affinity - Present today if brick resolves to client side
local host
** Needs Backport
** Can override using mount option
--xlator-option=volname.read-child=subvol-name

* NUFA
** Can use to replace DHT, alters default file placement to local
*** Backport of volgen enhancement
*** Wish-list - turn into mount-time option

Proposed Intermediate-term Glusterfs Enhancements
-------------------------------------------------

* Compression
** Move compression from Hadoop layer, into Glusterfs, to support
Unified use-case

* >64 node Scalability: Glusterd scalability
** Consider refactor of Glusterd to use standard state distribution and
coordination service

* >64 node Scalability: DHT and Directory span
** Question: What is the appropriate directory span for Hadoop data, do
all directories need to span all nodes
*** Implication: Ability to leverage currently latent glusterfs
directory span feature
** Consider moving Hash ranges currently distributed across all brick
directories into a mode centralized location to reduce number of
directory getxattrs
*** Option 1: A subset of all bricks contain hash ranges
*** Option 2: Share mappings across multiple directories (hash rrange
set, map or algorithmic) and only store pointer to mapping alg in
directory xattrs.

Other Potential Glusterfs Enhancements For Consideration
--------------------------------------------------------

* Replica-3
** Does Hadoop need support for 3-way JBOD config? If so, requires
*** Support for Replica-3 in AFR
*** Brick failure detection and orchestration of replace-brick
*** Management of spares pool
*** Bit-rot detection to proactively identify failing disks

* RPC Fast Path:
** Use case: protocol/client and protocol/server on same
** Consider use of Splice() or alternative to optimize transfer across
connection

* Striping
** Potential Setxattr interface for per-file chunksize and space

* NUFA
** Potential enhancement: Explore merging with DHT, select using
directory xattr
** Question: Would Hadoop benefit from a more generalized directed
placement mechanism?

* Disabling AFR Replication (temp-file use case)
** Add xattr to control pending flags, extend to use out-case
** When creating temp files, mark remote as dirty instead of replicating
data
** Self-heal on close if file not deleted.
** small code change

* At the Java/GlusterFileSystem Source Code level:
** Quick Slave I/O (hadoop) ~ decreased time for i/o by direct
connection to file system.
** Buffered Writes ~ decreased write time by use of buffers (avoids
context switching at the FUSE layer).

