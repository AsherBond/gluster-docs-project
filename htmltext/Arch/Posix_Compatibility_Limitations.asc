This page is intended to summaries differences between Glusterfs and
Strict POSIX behavior. Note: This page is a work in progress, and is
expected to build out over time.

Contents
~~~~~~~~

* link:#POSIX_Compatibility[1 POSIX Compatibility]
** link:#General_Glusterfs[1.1 General Glusterfs]
*** link:#Consistency[1.1.1 Consistency]
** link:#GeoRep[1.2 GeoRep]
** link:#UFO.2FSwift_Access[1.3 UFO/Swift Access]
* link:#General_Cross-Protocol_Interoperability_and_Unified_Access[2
General Cross-Protocol Interoperability and Unified Access]

POSIX Compatibility
-------------------

General Glusterfs
~~~~~~~~~~~~~~~~~

Consistency
^^^^^^^^^^^

* FUSE: Path to inode caching
** Currently FUSE, by default, cached mapping of directory path to inode
value for up to 1 second. In cases where a directory is deleted then
immediately recreated (rmdir <foo>Â ; mkdir <foo>), this can lead to
consistency issues when directory is accessed from other mount-points
during this 1 second interval. Note: FUSE is consistent for accesses
within the same mount point
*** Mitigation: Disable FUSE caching. Details TBD

* Items in
http://hekafs.org/index.php/2011/08/posix-limitations-in-fuse/[http://hekafs.org/index.php/2011/08/posix-limitations-in-fuse/]
(though the aux-gid issue has been addressed and inotify was never POSIX
to start with)

GeoRep
~~~~~~

* Current GeoRep uses path-based replication rather than
GFID/inode-based replication
** Implication: A file (inode) with multiple hard link on the Master
side results in separate files/inodes on the Slave side. Each Slave side
replica has distinct GFID
*** When file is changed on Master side, only replica corresponding to
path is updated on the Slave side. Potential for stale data within other
Slave-side replicas
** Mitigation:
*** Short-term avoid use of hard links for GeoReplicated data for files
that are RT/W, continued use for R/O data is OK
*** Longer-term: Proposed Parallel GeoRep design will move from
path-based to GFID-based replication, addressing the root-cause of the
issue.

UFO/Swift Access
~~~~~~~~~~~~~~~~

* Since OpenStack Swift object storage has the concept of distinct
immutable versions, each UFO Swift PUT operations result in new version
of file with distinct GFID.
** Implication: POSIX hard links apply to a specific version of an
object, rather than the most recent versions of an object. Object
updated via PUT only apply to path specified in PUT. Note: Since POST
operations update metadata for an existing version of an object, changes
are visible through all Posix paths to that object.

General Cross-Protocol Interoperability and Unified Access
----------------------------------------------------------

To Do: Create table based on earlier discussions

 +

FUSE

NFS

SMB

QEMU

Swift/UFO

GeoRep

Hadoop

FUSE

* Close to open consistency
* Coherent locks

NFSv3:

* Close to open consistency
* Compatible locks
* No ACLs in v3, partial implementation of Solaris hack

NFSv4:

* Single-side consistency (vs close-to-open)
* Caseless lookup
* Different ACLs
* Uplocks no supported (performance issue)
* No hard links in SMB

Note write-behind translator means close-to-open

* Implication: fsync required for live migration

General issue of object semantics to file semantics

* No Swift Versions
* No swift manifests

Eventual consistent

* No current support for hard links
** To be addressed with Parallel GeoRep
* No consistency (or tunable consistency) for open-for-write files

No support for compression

* Handled by Map/Reduce layer, limits unified access model
* To be addressed in figure

NFS

Limitations in concurrent r/w and write-write access across protocols

Limitations on cross-protocol file locking (needs QE) No hard links in
SMB

Similar to FUSE

Similar to FUSE

Similar to FUSE

No support support for anonymous fd's (short-term restriction)

Similar to FUSE

Same as FUSE

SMB

No Uplocks

Similar to FUSE

Eventual Consistency

Should work

Same as FUSE

QEMU

OK, subject to live migration guidelines

Only consistent if closed or snap

N/A

Swift

Same as POSIX, different GeoRep

Eventual Consistency

Should Work

Eventual Consistency

GeoRep

Subject to conflict resolution

Explore HBase compatibility (may require snap)

 +
 Opportunities to enhance performance if relaxed semantics:

* SMB:
** Improved caseless lookup
** Enhanced uplocks
* Swift/UFO: Object-only mode
** Large object count per containers
** Elimination of sub-containers as directories
* Hadoop:
** Shim implementation of Compression and Compression
** Delayed replication (instead of AFR)

