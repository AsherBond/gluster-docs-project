Contents
~~~~~~~~

* link:#Proposed_Approach[1 Proposed Approach]
* link:#Current_Prototype[2 Current Prototype]
** link:#Key_Differences_in_Proposed_Approach_from_Current_Prototype[2.1
Key Differences in Proposed Approach from Current Prototype]
* link:#Issues[3 Issues]

Proposed Approach
-----------------

Overall approach is to perform bit-rot detection on a per-brick basis,
with each node responsible for performing bit rot detection on all
storage local to the node.

Bit Rot Detection consists of two sets of processes

* A set of processes that sweeps the entire filesystem on a periodic
basis. performance.
* A set of processes that that trolls for changed files and computed
updated checksums. While this is optional, it minimizes the window in
which files go without checksum protection

Some considerations:

* Files may be undergoing active change at point of bit-rot comparison
or checksup update
** This can result is false positives. Multiple failed comparisons
required to detect a true instance of bit rot
** Deferring computation of checksum for a minimum stability period
after last write to file minimizes likelihood of wasted checksum
computation while a file is open
*** VM images and log files open for prolonged period of time may not
have sufficient stability period, may neeed to consider alternative
approaches.
* Care must be taken to throttle bit-rot detection to ensure that is
does not compromise steady-state
* Sorting files by local filesystem inode value and accessing in inode
order may minimize the potential for excessive head seeks during scan
for files.

General Approach:

* Per-file checksum value stored as extended attribute:
** Computed on a first-come, first-served basis by any node performing
bit-rot computation for a brick
*** Computation only performed when file reaches minimum stability value
*** In the case of striped files, checksum should be performed on a per
stripe-set value
* Per replica validation timestamp. Timestamp helps identify which brick
may contain rotten data, as well as potential time frame when rot
occured

Current Prototype
-----------------

Basic flow:

* <fill in here>
* Checksum: SHA256

Considerations:

* File needs to be mounted using a GUID such that xtime isn't changed
due to update of bitrot related xattrs

Extended Attributes Created/Used by BitRot:

* "trusted.glusterfs.bitrot.checksum"
** Format (JSON Structure):
*** v: <BitRotStructureVersion>
*** c: <checksum>
*** t: <timestamp>
*** b: <brick>
*** h: <hashtype>, where hashtype currently is SHA256

* "trusted.glusterfs.bitrot.validate.<n>"
** Where <n> corresponds to brick within replica set
** Format (JSON Structure):
*** v: <BitRotStructureVersion>
*** s: <status>
*** t: <timestamp>
*** b: <brick>
*** h: <hashtype>, where hashtype currently is SHA256

Outputs:

* List of skipped files
* List of Invalid files
* List of Missing Files
* stats: valid files, invalid files, updated files, skipped files,
missing files

Current Command line structure:

usage:Â %prog [options] volume_mountpoint

* "-i" -- update or validate checksums for files changed specified
xtime, when provided, or xtime xtime of last bitrot scan.
* "-x <min_time>" -- where <min_time> is minimum xtime value, where TIME
in sec, used to identify files and sub directories for ncremental
update. Typically time of last execution of update. If not specified,
xtime from last bitrot scan is used. Note: actual min xtime adjusted to
reflect file stabilization time
* "-j <jitter>" -- where: <joitter> is allowable variability in reported
xtime between replicas, where TIME in fractional sec
* "-s <min_stable>" -- where minimum stabilization time after file
modification, where TIME in sec
* "-r" -- prints out results from most recent full and incremental scans
* "-v" -- verified files flagged as invalid or missing in most recent
scans

Key Differences in Proposed Approach from Current Prototype
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* Overall approach will be GFID based rather than file based
* Full Brick Scanner
** Operates off GFID entries in brick [.glusterfs] directories
** Entries are first sorted in local inode order to optimize disk access
** Basic per inode processing loop:
*** Check Glusterfs file xattr to determine if file is currently open
(for write)?
**** If so, put GFID link in deferred directory
*** If closed (for write), compute checksum
*** If node current checksum, update
*** If current checksum, verify computed value against computed value
**** If mis-compare (and file still closed), then insert into error
directory
** On a periodic basis, retest any deferred GFID's
* Incremental Updates: Leverage common x-find tool for identification of
changed files
** currently uses marker framework to detect changes, will be upgraded
to use index translator changelog enhancements
* Add support for striped files
** Proposed approach: Compute separate checksum for each stripe. note as
index on checksum and verification xattrs
* Use index directories to capture file lists (missing, skipped, invalid

Issues
------

1.  Handling of striped files
2.  Detection of self/heal / outcast and avoidance of files in this
phase.
3.  Need unique GUID for BitRot
4.  How to we guarantee that data accessed is local
1.  Either special mode for AFR or directly access brick
2.  Since checksums computed on individual stripe members, need access
to individual stripes for SHA256 computation

