FIXME: *WORK IN PROGRESS*

New filesystems are constantly being developed to address the various
needs of the market, Once such filesystem is GlusterFS. GlusterFS from Z
Research Inc. (http://www.zresearch.com/[http://www.zresearch.com/]) is
one of the few Clustered Filesystems available for Linux today. This
article explains GlusterFS internals and characteristics, how to install
and configure it on Linux server and the operational experience of
GlusterFS at various sites.

A filesystem is used to store and manage user data on disk drives. It
ensures that the integrity of the data written to the disk is identical
to the data that is read back. In addition to storing data in files, a
filesystem also creates and manages information, such as free space and
inodes, about the filesystem itself. Filesystem structures commonly are
referred to as metadata, everything concerning a file except the actual
data inside the file. Elements of the file, such as its physical
location and size, are tracked by metadata.

Contents
~~~~~~~~

* #Clustered_Filesystem_v.2Fs_Distributed_Filesystem:[1 Clustered
Filesystem v/s Distributed Filesystem:]
* #Userspace_Filesystems:[2 Userspace Filesystems:]
* #GlusterFS_Introduction:[3 GlusterFS Introduction:]
* #History_of_GlusterFS:[4 History of GlusterFS:]
* #Installation_of_GlusterFS:[5 Installation of GlusterFS:]
** #Installation_of_GlusterFS_Server:[5.1 Installation of GlusterFS
Server:]
** #Installation_of_GlusterFS_Client:[5.2 Installation of GlusterFS
Client:]
* #Writing_Configuration_spec_files:[6 Writing Configuration spec
files:]
** #Beginner:[6.1 Beginner:]
** #Medium:[6.2 Medium:]
** #Expert:[6.3 Expert:]
* #GlusterFS_Internal_Details_.28in_brief.29:[7 GlusterFS Internal
Details (in brief):]
* #GlusterFS_experience_at_PDVSA_Cluster_.28Venezuela.29:[8 GlusterFS
experience at PDVSA Cluster (Venezuela):]
* #Conclusion:[9 Conclusion:]
* #Acknowledgements:[10 Acknowledgements:]

Clustered Filesystem v/s Distributed Filesystem:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

As file sizes and data sets grow into the terabyte and petabyte range,
users are looking for a method for storing, accessing and sharing the
files among different hosts.

That's where clustered file systems come in. Vendors have created
software and hardware appliances that combine disparate file systems
into one file system with one name space. These appliances and software
improve users' ability to access data and share the data with others
irrespective of the media or host computer on which it sits.

The technology these appliances and software use is known as clustered
file systems. File systems of these types have several advantages over
distributed file systems:

* By clustering systems and sharing applications and data, tasks can be
performed much more quickly than they could on individual machines
because data doesn't need to be copied or replicated from one file
system to another.

* Clustering provides more space for files and file systems.

* Management is easier because only one file system is being managed,
not a file system for each storage device or host computer.

* Failover is available because one server in the cluster can take over
the responsibilities of another if it fails.

* Users have concurrent access to all files located on the storage
devices on their network.

In a cluster, a group of independent nodes or host computers work
together as one system.They may share a common storage array and have a
common file system that has one name space. So the example here is
GlusterFS which is an Clustered Filesystem from which we can build High
Performance Compute clusters.

Userspace Filesystems:
~~~~~~~~~~~~~~~~~~~~~~

Historically userspace process management was put forward by projects
such as GNU/Hurd where PCI drivers, filesystems were written as
userspace process. GNU/Hurd was a microkernel so in theory for all the
microkernels other than base kernel all other subsystems will be
userspace process (SCSI subsystem, Network Subsystem.. etc)

Filesystems written without tainting the kernel, totally running as a
userspace process are called Userspace Filesystems (assuming to have a
bridge to communicate with Kernel space during operation).

Writing userspace filesystems in Linux has been a charm since the
inception of FUSE project
(http://fuse.sourceforge.net/[http://fuse.sourceforge.net/]). As you see
FUSE from Linux kernel acts as a bridge between the Userspace filesystem
and Kernel. Filesystems written with using FUSE are actually Virtual
Filesystems. Unlike traditional filesystems which essentially save data
to and retrieve data from disk, virtual file systems do not actually
store data themselves. They act as a view or translation of an existing
filesystem or storage device. In principle any resource available to
FUSE implementation can be exported as a file system.

GlusterFS Introduction:
~~~~~~~~~~~~~~~~~~~~~~~

GlusterFS is a clustered file system capable of scaling to several
peta-bytes. It aggregates various storage bricks over Infiniband RDMA or
TCP/IP interconnect into one large parallel network file system. Storage
bricks can be made of any commodity hardware such as x86-64 server with
SATA-II RAID and Infiniband HBA). GlusterFS is easily configurable, user
friendly and to best it is the most scalable filesystem available.
GlusterFS uses it's internal schedulers/translators combination to get
the much out of the system to provide the better performance. Most
intruiging part of GlusterFS is that it evovles from the idealogy of
"HURD kernel and userspace process management", so likewise GlusterFS is
a userspace process using backend of FUSE (Filesystem in UserSpace)
implementation of Linux Kernel.

History of GlusterFS:
~~~~~~~~~~~~~~~~~~~~~

GlusterFS project started as the part of Gluster Project
(http://www.gluster.org/[http://www.gluster.org/]) around July 2005. But
the first ever code got committed exactly one year later around July
2006. GlusterFS started with a design of high scalabity and performance
in mind. First ever DOA (Dead On Arrival) release was done during
September 2006, this version was just a release which showed the
capability and idealogy at which the GlusterFS pursued.

GlusterFS 1.2.3 stable release came around Feb 2007 which was first
stable release, benchmarks showed a good amount of performance but
neverthless not upto what was expected.

Later on there was a design change which led to the release of
1.3-VERGACION series. Which boosted the performance to a large extent
than the previous releases which really got the attention of theOpen
Source Community and various HPC experts. Till today there are good
amount of production and many test deployments around see
(http://gluster.org/docs/index.php/Who%27s_using_GlusterFS[http://gluster.org/docs/index.php/Who%27s_using_GlusterFS])
and lots of Community support in testing which helped in stabilizing
GlusterFS.

GlusterFS is fully POSIX complaint so that portabiity to other OSses
like BSD's, Solaris etc. is not a tedious task to take on.

So roughly having an history of more than a year the project is getting
ready for the production use. Adding to that much more features requests
from the community are on their way and various other performance boosts
with upcoming 1.4 release.

Installation of GlusterFS:
~~~~~~~~~~~~~~~~~~~~~~~~~~

Get a copy of the latest GlusterFS release from its ftp site at
http://ftp.zresearch.com/pub/gluster/glusterfs[http://ftp.zresearch.com/pub/gluster/glusterfs]

GlusterFS uses GNU Autotool build scripts for configuration and
building.

After downloading glusterfs first step you need to check if that you are
root, GlusterFS needs the user to be root (NOTE: if you are not root
like on Distributions like ubuntu use sudo) to begin the installation

GlusterFS depends on the following packages which need to be installed

* fuse 2.6 and higher (preferably
ftp://ftp.zresearch.com/pub/gluster/glusterfs/fuse[ftp://ftp.zresearch.com/pub/gluster/glusterfs/fuse]
a tuned version from the GlusterFS developers)
* libibverbs (this is optional)

While you are root untar the downloaded source tarball like this

----------------------------------
 # tar -xzf glusterfs-1.3.7.tar.gz
----------------------------------

---------------------
 # cd glusterfs-1.3.7
---------------------

-----------------------------------------------------
 # ./configure --prefix=/usr && make && make install 
-----------------------------------------------------

on Ubuntu and on other sudo driven distributions

------------------------------------
 # ./configure --prefix=/usr && make
 # sudo make install 
------------------------------------

 +
 --prefix=/usr is the prefix for the installation directory (NOTE: w/o
such an option GlusterFS will get install in /usr/local by default), you
can change this parameter like wise depending on the standard path where
the applications are installed.

Installation of GlusterFS Server:
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

During the above installation procedure we have encountered both server
and client install here is the procedure where you can install only
server with a simple configure option

---------------------------------------------------------------------------
 # ./configure --prefix=/usr  --disable-fuse-client && make && make install
---------------------------------------------------------------------------

Remember installation of GlusterFS server doesn't require fuse to be
installed.

Installation of GlusterFS Client:
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

This procedure is to only install only client, remember this requires to
fuse to be installed before proceding with this installation.

---------------------------------------------------------------------
 # ./configure --prefix=/usr --disable-server && make && make install
---------------------------------------------------------------------

Writing Configuration spec files:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Beginner:
^^^^^^^^^

Medium:
^^^^^^^

Expert:
^^^^^^^

GlusterFS Internal Details (in brief):
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

GlusterFS experience at PDVSA Cluster (Venezuela):
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Conclusion:
~~~~~~~~~~~

Acknowledgements:
~~~~~~~~~~~~~~~~~
