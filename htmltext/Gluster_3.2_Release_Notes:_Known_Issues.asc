http://www.gluster.org/community/documentation/index.php/Gluster_3.2_Release_Notes:_Compatibility_Issues[<<Compatibility
Issues] |
http://www.gluster.org/community/documentation/index.php/Gluster_3.2_Release_Notes[Table
of Contents] |
http://www.gluster.org/community/documentation/index.php/Gluster_3.2_Release_Notes:_Issues_Resolved[Issues
Resolved>>]

The following are the known issues:

* Issues related to Distributed Replicated Volumes:
** In GlusterFS replicated setup, if you are inside a directory (for
example, ‘Test’ directory) of replicated volume. From another node, you
will delete a file inside ‘Test’ directory. Then if you perform 'stat'
operation on the same file name, the file will be automatically created.
(that is, a proper directory self-heal is not triggered when process has
done ‘cd' into a path). +
 +
** Open fd self heal blocks the I/O on fd. +
While doing self-heal on open file descriptors in replicate, the I/O
operations on that particular file descriptor may get blocked. +
 +
** Manual intervention required in split-brain scenario. +
If a file is in a split-brain situation, then GlusterFS does not perform
any operation on that file. In such a case, administrator must manually
check and decide which of the replicated copy is genuine. +
 +

* Issues related to Distributed Volumes:
** Rename of all the cases are not covered after the introduction of
gfid. Expect EINVAL for rm -rf type of operations. +
There are some corner cases of distribute rename (for example, source
file is in one node, and the destination file is already a renamed file
which has a distribute linkfile for it) are not properly handled after
the 'gfid' based file identification has come in. It can cause some
operations after the rename to fail with "Invalid argument" error (like
rm -rf after a corner case rename). +
 +
** Rebalance does not happen if bricks are down.  +
 +
Currently while running rebalance, make sure all the bricks are in
operating/connected state.  +
 +
** Rebalance can happen to already filled sub-volume. +
Current algorithm of rebalance is not considering the free-space in the
target brick before migrating data. +
 +

* glusterfsd - Error return code is not proper after daemonizing the
process. +
 +
Due to this, scripts that mount glusterfs or start glusterfs process
must not depend on its return value. +

* glusterd - Parallel rebalance  +
 With the current rebalance mechanism, the machine issuing the rebalance
is becoming a bottleneck as all the data migrations are happening
through that machine.  +
 +
* Parallel operations (add brick, remove brick, and so on) with CLI from
different nodes can crash glusterd. +
 +

* After '# gluster volume replace-brick <VOLNAME> commit' command is
issued, the file system operations on that particular volume, which are
in transit will fail. +
 +

* Downgrading from 3.2.0 to 3.1.x  +
 If you are using 3.2.0, then the new features are enabled in the
default volume files (i.e. new translators). So after the downgrade, old
versions fail to recognize the new options/ translators and fail to
start. +
Work Around: Before starting the downgrade procedure, run the following
commands:  +
_#gluster volume reset <VOLNAME> force_ +
 _# gluster volume geo-replication stop MASTER SLAVE_ +
 +
 Now you can downgrade to 3.1.x. +
 +
Run any parameter changing operations on the volume. For example,
operations like _#gluster volume set <VOL> read-ahead off_ and _#gluster
volume set <VOL> read-ahead on_

* Issues related to Directory Quota:
** Some writes can appear to pass even though the quota limit is
exceeded (write returns success). This is because they could be cached
in write-behind. However disk-space would not exceed the quota limit,
since when writes to backend happen, quota does not allow them. Hence it
is advised that applications should check for return value of close
call. +
** Rename operation (that is, removing oldpath and creating newpath)
requires additional disk space equal to file size. This is because,
during rename, it subtracts the size on oldpath after rename operation
is performed, but it checks whether quota limit is exceeded on parents
of newfile before rename operation. +
** With striped volumes, Quota feature is not available. +
 +

* Issues related to Geo-replication:
** Password-less SSH is mandatory between Master and Slave nodes. +
 +

The following are few known missing (minor) features: +

* access-control - The application operations which require POSIX ACLs
to function properly will not work on GlusterFS. +
 +

* locks - 'mandatory' locking is not supported. +
 +

* NFS - NLM (Network Lock Manager) is not supported. +
 +

http://www.gluster.org/community/documentation/index.php/Gluster_3.2_Release_Notes:_Compatibility_Issues[<<Compatibility
Issues] |
http://www.gluster.org/community/documentation/index.php/Gluster_3.2_Release_Notes[Table
of Contents] |
http://www.gluster.org/community/documentation/index.php/Gluster_3.2_Release_Notes:_Issues_Resolved[Issues
Resolved>>]
