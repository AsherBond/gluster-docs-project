The clustering translators takes more than one subvolumes.

Contents
~~~~~~~~

* link:#Translator_cluster.2Fdistribute[1 Translator cluster/distribute]
* link:#Translator_cluster.2Fnufa[2 Translator cluster/nufa]
* link:#Translator_cluster.2Freplicate[3 Translator cluster/replicate]
* link:#Translator_cluster.2Fstripe[4 Translator cluster/stripe]
* link:#Translator_cluster.2Fha[5 Translator cluster/ha]
* link:#Obsolete.2Flegacy[6 Obsolete/legacy]
** link:#Translator_cluster.2Funify[6.1 Translator cluster/unify]
*** link:#GlusterFS_Schedulers[6.1.1 GlusterFS Schedulers]
**** link:#ALU_Scheduler[6.1.1.1 ALU Scheduler]
**** link:#NUFA_Scheduler[6.1.1.2 NUFA Scheduler]
**** link:#Random_Scheduler[6.1.1.3 Random Scheduler]
**** link:#Round-Robin_Scheduler[6.1.1.4 Round-Robin Scheduler]
**** link:#Switch_Scheduler[6.1.1.5 Switch Scheduler]

http://www.gluster.org/community/documentation/index.php/Translators/cluster/distribute[Translator
cluster/distribute]
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Distribute translator, or DHT, or simply, hash translator is designed
for O(1) scalability. This does not need any namespace translator, hence
for applications which use lot of small files, it will be a significant
improvement.

-------------------------------------------------------------
volume bricks
  type cluster/distribute
#  option lookup-unhashed yes
#  option min-free-disk 20%
  subvolumes brick1 brick2 brick3 brick4 brick5 brick6 brick7
end-volume
-------------------------------------------------------------

* lookup-unhashed

This option when provided, will make the dht translator act as a generic
cluster translator where it sends lookup call on all the subvolumes,
hence there will be no files missing over filesystem. This option is
very much useful when someone is shifting to 'cluster/distribute'
translator from 'cluster/unify' translator which was the default
clustering translator in earlier releases. Default option 'off'

 +

* min-free-disk

This option tells the 'cluster/distribute' volume to stop creating files
in the volume where the file gets hashed to, if the available disk space
is lesser than the given option. Default option '10%'

 +

* subvolumes

This option lists the subvolumes that are part of this
'cluster/distribute' volume. This translator _requires_ more than one
subvolume.

 +

http://www.gluster.org/community/documentation/index.php?title=Understanding_DHT_Translator&action=edit&redlink=1[Understanding
DHT Translator] for more technical details.

http://www.gluster.org/community/documentation/index.php/Translators/cluster/nufa[Translator
cluster/nufa]
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

NUFA translator or Non Uniform File Access translator is designed for
giving higher preference to local drive when used in a HPC type of
environment.

-------------------------------------------------------------
volume bricks
  type cluster/nufa
  option local-volume-name brick1
  subvolumes brick1 brick2 brick3 brick4 brick5 brick6 brick7
end-volume
-------------------------------------------------------------

* lookup-unhashed

This is an advanced option where files are looked up in all subvolumes
if they are missing on the subvolume matching the hash value of the
filename. The default is on.

* local-volume-name

The volume name to consider local and prefer file creations on. The
default is to search for a volume matching the hostname of the system.

* subvolumes

This option lists the subvolumes that are part of this 'cluster/nufa'
volume. This translator _requires_ more than one subvolume.

 +
 Refer
http://www.gluster.org/community/documentation/index.php?title=NUFA_with_single_process&action=edit&redlink=1[NUFA_with_single_process]
example for proper usage scenario with NUFA.

http://www.gluster.org/community/documentation/index.php/Translators/cluster/replicate[Translator
cluster/replicate]
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Replicate provides RAID-1 like functionality. Replicate keeps files and
directories on all subvolumes in sync. Hence if Replicate has four
subvolumes, there will be four copies of all files and directories. In
case one of the subvolumes goes down (i.e server crash, network
disconnection) Replicate will still service the requests from the
redundant copies.

Replicate also provides self-healing functionality. In case the crashed
servers comes up, the outdated files and directories will be updated
with the latest versions. Replicate uses the extended attributes of the
backend file system to track the pending activities over the files and
directories to provide the self-heal feature.

---------------------------------
volume afr-example
  type cluster/replicate
  subvolumes brick1 brick2 brick3
end-volume
---------------------------------

The above example volfile will replicate all directories and files on
brick1, brick2 and brick3. The subvolumes can be another translator

*NOTE:* Replicate needs *extended attribute* support in the underlying
FS, and also _'posix-locks'_ translator over the posix translator.

 +

* *read-subvolume*

The subvolume name on which read operations should be performed on. By
default read operations are balanced across subvolumes.

* *favorite-child*

In the event of a split brain, consider the data or file from
*favorite-child* subvolume to be the master copy while self-healing.

* *data-self-heal*, *metadata-self-heal* and *entry-self-heal*

Enable or disable self healing of file contents, metadata of the files
and directory entries respectively. The default is 'on' for all the
three.

* *data-change-log'_,_*_metadata-change-log_ and *entry-change-log*

Keep track of transaction log in the extended attributes of the files to
decide reliably the master copy at the time of self-heal. Disabling the
change-log will make Replicate pick a subvolume arbitrarily as the
master copy. Refer
http://www.gluster.org/community/documentation/index.php?title=Understanding_AFR_Translator&action=edit&redlink=1[Understanding
AFR Translator] for more details.

* *data-lock-server-count*, *metadata-lock-server-count* and
*entry-lock-server-count*

The number of servers to hold a lock on for each type of operation. The
default is 1 for all the three.

 +
 Refer to
*http://www.gluster.org/community/documentation/index.php?title=Understanding_AFR_Translator&action=edit&redlink=1[Understanding
AFR Translator]* to see more volume files, and understand the design.

http://www.gluster.org/community/documentation/index.php/Translators/cluster/stripe[Translator
cluster/stripe]
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The striping translator stripes files into given block-size (default
value is 128k) to its subvolumes (or child nodes).

*NOTE:* Stripe needs *extended attribute* support in the underlying FS.

-----------------------------------------
volume stripe
   type cluster/stripe
   option block-size 1MB
   subvolumes brick1 brick2 brick3 brick4
 end-volume
-----------------------------------------

* block-size

The block size in which file data is striped across subvolumes. Default
stripe size is 128KB.

http://www.gluster.org/community/documentation/index.php/Translators/cluster/ha[Translator
cluster/ha]
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

HA or High Availability translator provides the feature of fail over
mechanism between two volumes. It can be two servers exporting a big
clustered volume. It can be same server over two different (IB and TCP)
interfaces. This translator is still Beta for the 2.0 releases and will
be made available for GA in 2.1

----------------------------------
volume ha
  type cluster/ha
  subvolumes interface1 interface2
end-volume
----------------------------------

Obsolete/legacy
---------------

http://www.gluster.org/community/documentation/index.php/Translators/cluster/unify[Translator
cluster/unify]
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If you have some existing data, unify is the best translator. If your
setup is fresh, use the distribute translator. The unify translator
combines multiple storage bricks into one big fast storage server. For
I/O scheduling, you can bind your preferred I/O scheduler module to the
unify volume. You have a variety of I/O schedulers to pick from, based
on your application requirements.

Read
http://www.gluster.org/community/documentation/index.php?title=Understanding_Unify_Translator&action=edit&redlink=1[Understanding
Unify Translator] to know more about the unify translator.

------------------------------------------------------------------------------------
volume unify
   type cluster/unify
   subvolumes brick1 brick2 brick3 brick4 brick5 brick6 brick7 brick8
   option namespace brick-ns # should be a node which is not present in 'subvolumes'
   option scheduler rr    # simple round-robin scheduler
end-volume
------------------------------------------------------------------------------------

 +

GlusterFS Schedulers
^^^^^^^^^^^^^^^^^^^^

The scheduler decides how to distribute the new creation operations
across the clustered file system based on load, availability and other
determining factors. Here is a list of I/O schedulers you can pick
from...

ALU Scheduler
+++++++++++++

ALU stands for "Adaptive Least Usage". It is the most advanced scheduler
available in GlusterFS:

* It balances the load across volumes taking several factors into
account.
* It adapts itself to changing I/O patterns according to its
configuration.

When properly configured, it can eliminate the need for regular tuning
of the filesystem to keep volume load nicely balanced.

The ALU scheduler is composed of multiple least-usage sub-schedulers.
Each sub-scheduler keeps track of a certain type of load, for each of
the subvolumes, getting the actual statistics from the subvolumes
themselves. The sub-schedulers are these:

* *disk-usage* - the used and free disk space on the volume
* *read-usage* - the amount of reading done from this volume
* *write-usage* - the amount of writing done to this volume
* *open-files-usage* - the number of files currently opened from this
volume
* *disk-speed-usage* - the speed at which the disks are spinning. This
is a constant value and therefore not very useful.

The ALU scheduler needs to know which of these sub-schedulers to use and
in which order to evaluate them. This is done through the "*option
alu.order*" configuration directive.

Each sub-scheduler needs to know two things: when to kick in (the
*entry-threshold*), and how long to stay in control (the
*exit-threshold*). For example: when unifying three disks of 100GB,
keeping an exact balance of disk-usage is not necessary. Instead, there
could be a 1GB margin which can be used to nicely balance other factors,
such as read-usage. The disk-usage scheduler can be told to kick in only
when a certain threshold of discrepancy is passed, such as 1GB. When it
assumes control under this condition, it will write all subsequent data
to the least-used volume. If it is doing so, it is unwise to stop right
after the values are below the entry-threshold again since that would
make it very likely that the situation will occur again very soon. Such
a situation would cause the ALU to spend most of its time disk-usage
scheduling which is unfair to the other sub-schedulers. The
exit-threshold therefore defines the amount of data that needs to be
written to the least-used disk before control is relinquished again.

In addition to the sub-schedulers, the ALU scheduler also has "*limits*"
options. These can stop the creation of new files on a volume once
values drop below a certain threshold. For example, setting "*option
alu.limits.min-free-disk* 5GB" will stop the scheduling of files to
volumes that have less than 5GB of free disk space leaving the files on
that disk some room to grow.

The actual values you assign to the thresholds for sub-schedulers and
limits depend on your situation. If you have fast-growing files, you
would want to stop file-creation on a disk much earlier than when hardly
any of your files are growing. If you care less about disk-usage balance
than about read-usage balance, you would want a bigger disk-usage
scheduler entry-threshold and a smaller read-usage scheduler
entry-threshold.

For thresholds defining a size, percentage of free space is allowed. For
example: "option alu.limits.min-free-disk 5%".

* *ALU Scheduler Volume example*

-----------------------------------------------------------------------------------------------------------------------------
volume bricks
  type cluster/unify
  subvolumes brick1 brick2 brick3 brick4 brick5
  option alu.read-only-subvolumes brick5 # This option makes brick5 to be readonly, where no new files are created.
  option scheduler alu   # use the ALU scheduler
  option alu.limits.min-free-disk  5%      # Don't create files one a volume with less than 5% free diskspace
  option alu.limits.max-open-files 10000   # Don't create files on a volume with more than 10000 files open
  
  # When deciding where to place a file, first look at the disk-usage, then at  
  # read-usage, write-usage, open files, and finally the disk-speed-usage.
  option alu.order disk-usage:read-usage:write-usage:open-files-usage:disk-speed-usage
  option alu.disk-usage.entry-threshold 2GB   # Kick in if the discrepancy in disk-usage between volumes is more than 2GB
  option alu.disk-usage.exit-threshold  60MB   # Don't stop writing to the least-used volume until the discrepancy is 1988MB 
  option alu.open-files-usage.entry-threshold 1024   # Kick in if the discrepancy in open files is 1024
  option alu.open-files-usage.exit-threshold 32   # Don't stop until 992 files have been written the least-used volume
# option alu.read-usage.entry-threshold 20%   # Kick in when the read-usage discrepancy is 20%
# option alu.read-usage.exit-threshold 4%   # Don't stop until the discrepancy has been reduced to 16% (20% - 4%)
# option alu.write-usage.entry-threshold 20%   # Kick in when the write-usage discrepancy is 20%
# option alu.write-usage.exit-threshold 4%   # Don't stop until the discrepancy has been reduced to 16%
# option alu.disk-speed-usage.entry-threshold # NEVER SET IT. SPEED IS CONSTANT!!!
# option alu.disk-speed-usage.exit-threshold  # NEVER SET IT. SPEED IS CONSTANT!!!
  option alu.stat-refresh.interval 10sec   # Refresh the statistics used for decision-making every 10 seconds
# option alu.stat-refresh.num-file-create 10   # Refresh the statistics used for decision-making after creating 10 files
end-volume
-----------------------------------------------------------------------------------------------------------------------------

NUFA Scheduler
++++++++++++++

Non-Uniform Filesystem Scheduler similar to NUMA
(http://en.wikipedia.org/wiki/Non-Uniform_Memory_Access[http://en.wikipedia.org/wiki/Non-Uniform_Memory_Access])
memory design. It is mainly used in HPC environments where you are
required to run the filesystem server and client within the same
cluster. Under such environment, NUFA scheduler gives the local system
more priority for file creation over other nodes.

----------------------------------------------------------
volume posix1
  type storage/posix               # POSIX FS translator
  option directory /home/export    # Export this directory
end-volume 

volume bricks
  type cluster/unify
  subvolumes posix1 brick2 brick3 brick4
  option scheduler nufa
  option nufa.local-volume-name posix1
  option nufa.limits.min-free-disk 5%
end-volume
----------------------------------------------------------

*NOTE:* Now NUFA comes with support for more than one local volume
option.

Random Scheduler
++++++++++++++++

Random scheduler randomly scatters file creation across storage bricks.

----------------------------------------
volume bricks
  type cluster/unify
  subvolumes brick1 brick2 brick3 brick4
  option scheduler random
  option random.limits.min-free-disk 5%
end-volume
----------------------------------------

Round-Robin Scheduler
+++++++++++++++++++++

Round-Robin (RR) scheduler creates files in a round-robin fashion. Each
client will have its own round-robin loop. When your files are mostly
similar in size and I/O access pattern, this scheduler is a good choice.
RR scheduler now checks for free disk size of the server before
scheduling, so you can get to know when to add another server brick. The
default value of min-free-disk is _5%_ and is checked every 10seconds
(by default) if there is any create call happening.

-------------------------------------------------------------------------------
volume bricks
  type cluster/unify
  subvolumes brick1 brick2 brick3 brick4
  option scheduler rr
  option rr.read-only-subvolumes brick4  # No files will be created in 'brick4'
  option rr.limits.min-free-disk 5%          # Unit in %
  option rr.refresh-interval 10               # Check server brick after 10s.
end-volume
-------------------------------------------------------------------------------

 +

Switch Scheduler
++++++++++++++++

Switch Scheduler is the latest addition to the GlusterFS code base,
which actually schedules the file according the the filename patterns
specified. One can understand it with the example given below.

--------------------------------------------------------------------------
volume bricks
  type cluster/unify
  subvolumes brick1 brick2 brick3 brick4 brick5 brick6 brick7
  option scheduler switch
  option switch.case *jpg:brick1,brick2;*mpg:brick3;*:brick4,brick5,brick6
  option switch.read-only-subvolumes brick7
end-volume
--------------------------------------------------------------------------

Above is the snapshot of just unify translator in a spec file. Here,
files with pattern '*jpg' will be created in brick1 and brick2, and
'*mpg' will be created in brick3, and all other files will be created in
brick4,brick5, and brick6. And brick7 will be just read-only subvolume,
from which just data can be read.
