(http://www.gluster.org/community/documentation/index.php/Planning34[Back
to 3.4 planning page]?)

Contents
~~~~~~~~

* link:#Feature_ElasticBrick[1 Feature ElasticBrick]
** link:#1_Summary[1.1 1 Summary]
** link:#2_Owners[1.2 2 Owners]
** link:#3_Current_status[1.3 3 Current status]
** link:#4_Detailed_Description[1.4 4 Detailed Description]
** link:#5_Benefit_to_GlusterFS[1.5 5 Benefit to GlusterFS]
** link:#6._Scope[1.6 6. Scope]
*** link:#6.1._Nature_of_proposed_change[1.6.1 6.1. Nature of proposed
change]
*** link:#6.2._Implications_on_manageability[1.6.2 6.2. Implications on
manageability]
*** link:#6.3._Implications_on_presentation_layer[1.6.3 6.3.
Implications on presentation layer]
*** link:#6.4._Implications_on_persistence_layer[1.6.4 6.4. Implications
on persistence layer]
*** link:#6.5._Implications_on_.27GlusterFS.27_backend[1.6.5 6.5.
Implications on 'GlusterFS' backend]
*** link:#6.6._Modification_to_GlusterFS_metadata[1.6.6 6.6.
Modification to GlusterFS metadata]
*** link:#6.7._Implications_on_.27glusterd.27[1.6.7 6.7. Implications on
'glusterd']
** link:#7_How_To_Test[1.7 7 How To Test]
** link:#8_User_Experience[1.8 8 User Experience]
** link:#9_Dependencies[1.9 9 Dependencies]
** link:#10_Documentation[1.10 10 Documentation]
** link:#11_Status[1.11 11 Status]
** link:#12_Comments_and_Discussion[1.12 12 Comments and Discussion]

Feature ElasticBrick
--------------------

1 Summary
~~~~~~~~~

Adding bricks to an existing volume with replication is currently
problematic. Given a volume on 2 servers with replicate = 2, adding a
third server is only possible by having multiple bricks on the initial 2
servers. Example, node0 and node1 might each have 2 bricks. Adding a
third server node2 with another 2 bricks is possible but complicated.
Additionally, the rebalance will result in most of the data being moved
to keep hashes consistant.

An alternative would be to create 10 or 100 bricks for each node.
Assuming there is only two nodes at the start, adding another node (or
more) could instead be a replace-brick operation.

An ElasticBrick would effectively do this internally so adding and
removing nodes would not require a crazy amount of planning to make
everything balance and replicate correctly.

2 Owners
~~~~~~~~

This idea was hashed out at the Gluster Developer summit at RedHat in
Mountain View by Jeff Darcy, John Walker, and Bryan Whitehead.

3 Current status
~~~~~~~~~~~~~~~~

Concept stage.

4 Detailed Description
~~~~~~~~~~~~~~~~~~~~~~

A single "brick" on a node would internally be broken up into many
smaller bricks. These smaller bricks would keep a consistant hash as
nodes are added and removed keeping the rebalance less of a nightmare.

5 Benefit to GlusterFS
~~~~~~~~~~~~~~~~~~~~~~

Gluster is targeting a market where reliable (replicated) storage is
needed but with constrained budgets. This means a typical customer would
start off with few nodes and add them as needed.

Additionally, this goes along with the idea of Gluster "scaling
linearly". Easily adding nodes to existing and running volumes with
minimal administration juggling with bricks makes the sell more easy.
Currently just adding a node is a pretty big undertaking.

6. Scope
~~~~~~~~

6.1. Nature of proposed change
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Unknown

6.2. Implications on manageability
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Should simplify the overall administration. An existing volume would
just get another node:/brick combination added (or removed) and a
reshuffling of internal bricks would commence.

6.3. Implications on presentation layer
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Presentation layer should not be aware of this change (statement from a
non-gluster dev)

6.4. Implications on persistence layer
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Unknown.

6.5. Implications on 'GlusterFS' backend
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Maybe. Bricks would now need to keep track of N number of internal
elastic bricks.

6.6. Modification to GlusterFS metadata
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Maybe. See 6.5

6.7. Implications on 'glusterd'
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Maybe. See 6.5

7 How To Test
~~~~~~~~~~~~~

Create volume with replication=2 across two nodes. Add another node and
rebalance. Repeat. Remove one of the original nodes.

8 User Experience
~~~~~~~~~~~~~~~~~

CLI would effectively be the same. Documentation would need to change to
"worry about node:/brick order in a volume" to "gluster takes care of
it". rebalance status should reflect status of elastic-brick movement.

9 Dependencies
~~~~~~~~~~~~~~

Unknown.

10 Documentation
~~~~~~~~~~~~~~~~

Should be an overall simplification of Documentation. If the initial
number of elastic bricks is fixed an explanation of the maximum number
of nodes that could be added should be explained.

 +

11 Status
~~~~~~~~~

Concept stage.

12 Comments and Discussion
~~~~~~~~~~~~~~~~~~~~~~~~~~

http://www.gluster.org/community/documentation/index.php?title=User:Jiqiren&action=edit&redlink=1[Jiqiren]
(http://www.gluster.org/community/documentation/index.php?title=User_talk:Jiqiren&action=edit&redlink=1[talk])
11:18, 24 July 2012 (PDT)
