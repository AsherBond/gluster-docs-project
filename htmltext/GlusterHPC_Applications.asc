Contents
~~~~~~~~

* link:#mvapich-gen2[1 mvapich-gen2]
* link:#mpich2[2 mpich2]
* link:#openib[3 openib]
* link:#torque[4 torque]
* link:#c3[5 c3]
* link:#slurm[6 slurm]
* link:#conman[7 conman]
* link:#pdsh[8 pdsh]
* link:#lam[9 lam]
* link:#cerebro[10 cerebro]
* link:#genders[11 genders]
* link:#ganglia[12 ganglia]
* link:#autologin[13 autologin]

mvapich-gen2
~~~~~~~~~~~~

_Project Home Page_ :
http://nowlab.cse.ohio-state.edu/projects/mpi-iba/[http://nowlab.cse.ohio-state.edu/projects/mpi-iba/]

_Brief Description of the package_ :

MVAPICH is a high performance implementation of MPI-1 over InfiniBand
based on MPICH1. MVAPICH2 is a high performance MPI-2 implementation
based on MPICH2. These packages are implemented over the Verbs Level
Interface (VAPI). Recently released MVAPICH-Gen2 package also supports
the OpenIB/Gen2 interface.

mpich2
~~~~~~

_Project Home Page_ :
http://www-unix.mcs.anl.gov/mpi/mpich/[http://www-unix.mcs.anl.gov/mpi/mpich/]

_Documentation_ :
http://www-unix.mcs.anl.gov/mpi/mpich/index.html[http://www-unix.mcs.anl.gov/mpi/mpich/index.html]

_Brief Description of the package_ :

MPICH2 is an all-new implementation of MPI, designed to support research
into high-performance implementations of MPI-1 and MPI-2 functionality.
In addition to the features in MPICH, MPICH2 includes support for
one-side communication, dynamic processes, intercommunicator collective
operations, and expanded MPI-IO functionality. Clusters consisting of
both single-processor and SMP nodes are supported. With the exception of
users requiring the communication of heterogeneous data, we strongly
encourage everyone to consider switching to MPICH2. Researchers
interested in using using MPICH as a base for their research into MPI
implementations should definitely use MPICH2.

openib
~~~~~~

_Project Home Page_ : http://www.openib.org[http://www.openib.org]

_Documentation_ :
http://www.openib.org/doc.html[http://www.openib.org/doc.html]

_Brief Description of the package_ :

Infiniband userspace libraries and tools

torque
~~~~~~

_Project Home Page_ :
http://www.clusterresources.com/pages/products/torque-resource-manager.php[http://www.clusterresources.com/pages/products/torque-resource-manager.php]

_Documentation_ :
http://www.clusterresources.com/pages/resources/documentation.php[http://www.clusterresources.com/pages/resources/documentation.php]

_Brief Description of the package_ :

TORQUE (Tera-scale Open-source Resource and QUEue manager) is a resource
manager providing control over batch jobs and distributed compute nodes.
TORQUE is based on *OpenPBS version 2.3.12 and incorporates scalability,
fault tolerance, and feature extension patches provided by NCSA, OSC,
the U.S. Dept of Energy, Sandia, PNNL, U of Buffalo, TeraGrid, and many
other leading edge HPC organizations. This version may be freely
modified and redistributed subject to the constraints of the included
license. TORQUE provides enhancements over standard OpenPBS in the
following areas:

* Fault Tolerance
** Additional failure conditions checked/handled
** Many, many bugs fixed
** Node health check script support
* Scheduling Interface
** Extended query interface providing the scheduler with additional and
more accurate information
** Extended control interface allowing the scheduler increased control
over job behavior and attributes
** Allows the collection of statistics for completed jobs
* Scalability
** Significantly improved server to MOM communication model
** Ability to handle larger clusters (over 15 TF/2,500 processors)
** Ability to handle larger jobs (over 2000 processors)
** Ability to support larger server messages
* Usability
** Extensive logging additions
** More human readable logging (ie no more 'error 15038 on command 42')

c3
~~

Cluster Command & Control suite

_Project Home Page_ :
http://www.csm.ornl.gov/torc/C3/[http://www.csm.ornl.gov/torc/C3/]

_Documentation_ :
http://www.csm.ornl.gov/torc/C3/C3documentation.shtml[http://www.csm.ornl.gov/torc/C3/C3documentation.shtml]

_Brief Description of the package_ :

C3 is a command line interface that may also be called within programs.
C2G provides a Python/TK based GUI, that among other features, may
invoke the C3 tools. The Cluster Command and Control (C3) tool suite was
developed for use in operating the HighTORC Linux cluster at Oak Ridge
National Laboratory. This suite implements a number of command line
based tools that have been shown to increase system manager scalability
by reducing time and effort to operate and manage the cluster.

slurm
~~~~~

LLNL's Simple Utility for Resource Management

SLURM: A Highly Scalable Resource Manager

_Project Home Page_ :
http://www.llnl.gov/linux/slurm/slurm.html[http://www.llnl.gov/linux/slurm/slurm.html]

_Documentation_ :
http://www.llnl.gov/linux/slurm/documentation.html[http://www.llnl.gov/linux/slurm/documentation.html]

_Brief Description of the package_ :

SLURM is an open-source resource manager designed for GNU/Linux clusters
of all sizes. It provides three key functions. First it allocates
exclusive and/or non-exclusive access to resources (computer nodes) to
users for some duration of time so they can perform work. Second, it
provides a framework for starting, executing, and monitoring work
(typically a parallel job) on a set of allocated nodes. Finally, it
arbitrates conflicting requests for resources by managing a queue of
pending work.

SLURM is not a sophisticated batch system, but it does provide an
Applications Programming Interface (API) for integration with external
schedulers such as The Maui Scheduler. While other resource managers do
exist, SLURM is unique in several respects:

* Its source code is freely available under the GNU General Public
License.
* It is designed to operate in a heterogeneous cluster with up to 16,384
nodes.
* It is portable; written in C with a GNU autoconf configuration engine.
While initially written for GNU/Linux, other UNIX-like operating systems
should be easy porting targets. A plugin mechanism exists to support
various interconnects, authentication mechanisms, schedulers, etc.
* SLURM is highly tolerant of system failures, including failure of the
node executing its control functions.
* It is simple enough for the motivated end user to understand its
source and add functionality.

SLURM provides resource management on about 1000 computers world-wide
including many of the most powerful computers in the world including:

* BlueGene/L with 65,536 dual-processor compute nodes
* Thunder a GNU/Linux cluster with 1024 nodes each having four Itanium2
processors
* ASC Purple an IBM SP/AIX cluster with about 1500 nodes each having
eight Power5 processors

conman
~~~~~~

LLNL's console manager

_Project Home Page_ :
http://www.llnl.gov/linux/conman/[http://www.llnl.gov/linux/conman/]

_Brief Description of the package_ :

ConMan is a console management program designed to support a large
number of console devices and simultaneous users. It currently supports
local serial devices and remote terminal servers (via the Telnet
protocol).

ConMan's features include:

* Mapping symbolic names to console devices.
* Logging all output from a console device to file.
* Supporting monitor (R/O), interactive (R/W), and broadcast (W/O) modes
of console access.
* Allowing clients to join or steal console "write" privileges.
* Executing Expect scripts across multiple consoles in parallel.

pdsh
~~~~

LLNL's Parallel and distributed shell

_Project Home Page_ :
http://www.llnl.gov/linux/pdsh[http://www.llnl.gov/linux/pdsh]

_Brief Description of the package_ :

Pdsh is a high-performance, parallel remote shell utility. It has
built-in, thread-safe clients for Berkeley and Kerberos V4 rsh, and can
call SSH externally (though with reduced performance). Pdsh uses a
"sliding window" parallel algorithm to conserve socket resources on the
initiating node and to allow progress to continue while timeouts occur
on some connections.

Pdsh is similar to DSH, part of the IBM PSSP software offering, but
offers improved performance and handling of error conditions. It runs on
a variety of platforms (including as a replacement for DSH on AIX/PSSP),
but is primarily developed on GNU/Linux.

Recently, pdsh has been given the ability to run MPI jobs on the
Quadrics Elan3 interconnect under GNU/Linux.

lam
~~~

_Project Home Page_ : http://www.lam-mpi.org[http://www.lam-mpi.org]

_Documentation_ :
http://www.lam-mpi.org/using/docs[http://www.lam-mpi.org/using/docs]

_Brief Description of the package_ :

LAM/MPI is a high-quality open-source implementation of the Message
Passing Interface specification, including all of MPI-1.2 and much of
MPI-2. Intended for production as well as research use, LAM/MPI includes
a rich set of features for system administrators, parallel programmers,
application users, and parallel computing researchers. With LAM/MPI, a
dedicated cluster or an existing network computing infrastructure can
act as a single parallel computer. LAM/MPI is considered to be "cluster
friendly", in that it offers daemon-based process startup/control as
well as fast client-to-client message passing protocols. LAM/MPI can use
TCP/IP and/or shared memory for message passing (currently, different
RPMs are supplied for this -- see the main LAM web site for details).
Compliant applications are source code portable between LAM/MPI and any
other implementation of MPI. In addition to providing a high-quality
implementation of the MPI standard, LAM/MPI offers extensive monitoring
capabilities to support debugging. Monitoring happens on two levels.
First, LAM/MPI has the hooks to allow a snapshot of process and message
status to be taken at any time during an application run. This snapshot
includes all aspects of synchronization plus datatype maps/signatures,
communicator group membership, and message contents (see the XMPI
application on the main LAM web site). On the second level, the MPI
library is instrumented to produce a cumulative record of communication,
which can be visualized either at runtime or post-mortem.

cerebro
~~~~~~~

LLNL's Cluster monitor tools and libraries

_Project Home Page_ :
http://www.llnl.gov/linux/cerebro/cerebro.html[http://www.llnl.gov/linux/cerebro/cerebro.html]

_Brief Description of the package_ :

Cerebro is a collection of cluster monitoring tools and libraries. This
project has been named after a part of the central nervous system to pay
homage to the Ganglia project. A heavily modified version of Ganglia was
once used at Lawrence Livermore National Laboratory, but as needs and
demands changed, it became necessary to develop a slightly different
tool.

Several of goals of Cerebro were to develop a monitoring tool that

* Interrupts the CPU less frequently.
* Provides the user with a set of libraries, command-line tools, and a
dynamic module interface that allow users to monitor new metrics without
any re-compilation or configuration changes.
* Provides a dynamic module interface that allows individual clusters to
monitor different metrics based solely on the modules installed.
* Provides a dynamic module interface that allows the libraries and
tools to have knowledge of every node in the cluster.
* Provides a dynamic module interface that allows the libraries and
tools to automatically configure themselves across a cluster.
* Removes XML overhead.

genders
~~~~~~~

LLNL's static cluster configuration database management tools

_Project Home Page_ :
http://www.llnl.gov/linux/genders/[http://www.llnl.gov/linux/genders/]

_Brief Description of the package_ :

Cluster configuration management tools, main features being:

Nodeattr, the genders query tool, can be called upon to provide a list
of nodes that have a particular attribute, which can be fed into other
tools such as Pdsh. It is also commonly called from system
administration scripts to test whether a node has a particular
characteristic before performing some operation (for example, nodes with
the "qla2200" attribute might need to modprobe the qla2200 kernel module
in the rc.local file, while others do not).

Dist2, the rdist distfile preprocessor, expands specially formatted
Rdist macros with embedded genders attributes into node lists. [Dist2
also assumes a particular file system setup for rdisted files and their
Distfiles, described in the accompanying documentation]. When the
genders file changes, Dist2 can simply be rerun to redistribute
appropriate configuration file variations. Dist2 also facilitates rapid
"localization" of nodes rejoining the cluster after a fresh operating
system install, or a long period of absence.

ganglia
~~~~~~~

Distributed monitoring system

_Project Home Page_ :
http://ganglia.sourceforge.net[http://ganglia.sourceforge.net]

_Documentation_ :
http://ganglia.sourceforge.net/docs[http://ganglia.sourceforge.net/docs]

_Brief Description of the package_ :

Ganglia is a scalable distributed monitoring system for high-performance
computing systems such as clusters and Grids. It is based on a
hierarchical design targeted at federations of clusters. It leverages
widely used technologies such as XML for data representation, XDR for
compact, portable data transport, and RRDtool for data storage and
visualization. It uses carefully engineered data structures and
algorithms to achieve very low per-node overheads and high concurrency.
The implementation is robust, has been ported to an extensive set of
operating systems and processor architectures, and is currently in use
on thousands of clusters around the world. It has been used to link
clusters across university campuses and around the world and can scale
to handle clusters with 2000 nodes.

autologin
~~~~~~~~~

_Brief Description of the package_ :

Package which enables autologin between all the installed machines
Autologin is a simple package in GlusterHPC, added as an extension. By
installing this package, the whole cluster (installed using GlusterHPC)
will be configured to passwordless authentication for login (for ssh,
scp).
