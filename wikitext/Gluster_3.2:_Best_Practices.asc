Gluster 3.2: Restoring Data from the Slave[ Restoring Data from the
Slave] | link:Gluster 3.2 Filesystem Administration Guide[ Table of
Contents] | Gluster 3.2: Monitoring your GlusterFS Workload[ Monitoring
your GlusterFS Workload>>]

*Manually Setting Time*

If you have to change the time on your bricks manually, then you must
set uniform time on all bricks. This avoids the out-of-time sync issue
describedGluster 3.2: Setting Up the Environment for Geo-replication[
previously]. Setting time backward corrupts the geo-replication index,
so the recommended way to set the time manually is:

1.  Stop geo-replication between the master and slave using the
following command: +
 +
# gluster volume geo-replication stop
2.  Stop the geo-replication indexing using the following command: +
 +
# gluster volume set geo-replication.indexing off
3.  Set uniform time on all bricks
4.  Restart your geo-replication sessions by using the following
command: +
 +
# gluster volume geo-replication start

*Running Geo-replication commands in one system* +
 +
It is advisable to run the geo-replication commands in one of the bricks
in the trusted storage pool. This is because, the log files for the
geo-replication session would be stored in the *Server* where the
Geo-replication start is initiated. Hence it would be easier to locate
the log-files when required.

*Isolation* +
 +
Geo-replication slave operation is not sandboxed as of now and is ran as
a privileged service. So for the security reason, it is advised to
create a sandbox environment (dedicated machine / dedicated virtual
machine / chroot/container type solution) by the administrator to run
the geo-replication slave in it. Enhancement in this regard will be
available in follow-up minor release.

Gluster 3.2: Restoring Data from the Slave[ Restoring Data from the
Slave] | link:Gluster 3.2 Filesystem Administration Guide[ Table of
Contents] | Gluster 3.2: Monitoring your GlusterFS Workload[ Monitoring
your GlusterFS Workload>>]
