Gluster 3.2: Rotating Logs[ << Rotating Logs] |
link:Gluster 3.2 Filesystem Administration Guide[ Table of Contents]
|Gluster 3.2: Troubleshooting NFS[ Troubleshooting NFS>>]

This section describes the most common troubleshooting issues related to
GlusterFS Geo-replication.

*Locating Log Files*

For every Geo-replication session, the following three log files are
associated to it (four, if slave is a gluster volume):

* Master-log-file - log file for the process which monitors the Master
volume
* Slave-log-file - log file for process which initiates the changes in
slave
* Master-gluster-log-file - log file for the maintenance mount point
that Geo-replication module uses to monitor the master volume
* Slave-gluster-log-file - is the slave's counterpart of it

*Master Log File*

To get the Master-log-file for geo-replication, use the following
command: +
 gluster volume geo-replication config log-file +
 +
For example: +
 # gluster volume geo-replication Volume1 example.com:/data/remote_dir
config log-file +
 +
_' Slave Log File_'

To get the log file for Geo-replication on slave (glusterd must be
running on slave machine), use the following commands: +
# On master, run the following command: +
 # gluster volume geo-replication Volume1 example.com:/data/remote_dir
config session-owner +
 5f6e5200-756f-11e0-a1f0-0800200c9a66 +
 Displays the session owner details.

1.  On slave, run the following command: +
 # gluster volume geo-replication /data/remote_dir config log-file +

/var/log/gluster/$\{session-owner}:remote-dir.log +
# Replace the session owner details (output of Step 1) to the output of
the Step 2 to get the location of the log file. +
 /var/log/gluster/5f6e5200-756f-11e0-a1f0-0800200c9a66:remote-dir.log +
 *Synchronization is not complete*

*Description:* GlusterFS Geo-replication did not synchronize the data
completely but still the geo-replication status display OK.

*Solution:* You can enforce a full sync of the data by erasing the index
and restarting GlusterFS Geo-replication. After restarting, GlusterFS
Geo-replication begins synchronizing all the data, that is, all files
will be compared with by means of being checksummed, which can be a
lengthy /resource high utilization operation, mainly on large data sets
(however, actual data loss will not occur). If the error situation
persists, contact Gluster Support.

For more information about erasing index, see
Gluster 3.2: Tuning Volume Options[ Tuning Volume Options].

*Issues in Data Synchronization*

*Description:* GlusterFS Geo-replication display status as OK, but the
files do not get synced, only directories and symlink gets synced with
error messages in log:

[2011-05-02 13:42:13.467644] E [master:288:regjob] GMaster: failed to
sync ./some_file`

*Solution:* GlusterFS Geo-replication invokes rsync v3.07 in the host
and the remote machine, check if you have the desired version installed.

11.2.4. Geo-replication status displays Faulty very often

Description: GlusterFS Geo-replication display status as faulty, very
often with a backtrace similar to the following: +
 2011-04-28 14:06:18.378859] E [syncdutils:131:log_raise_exception] : +
FAIL: +
Traceback (most recent call last): +
File "/usr/local/libexec/glusterfs/python/syncdaemon/syncdutils.py",
line 152, in twraptf(*aa) +
File "/usr/local/libexec/glusterfs/python/syncdaemon/repce.py", line
118, in listen rid, exc, res = recv(self.inf) +
File "/usr/local/libexec/glusterfs/python/syncdaemon/repce.py", line 42,
in recv return pickle.load(inf) +
EOFError +
 *Solution:* This means that the RPC communication between the master
gsyncd module and slave gsyncd module is broken and this can happen for
various reasons. Check if it satisfies all the following pre-requisites:

* Password-less SSH is set up properly between the host and the remote
machine.
* If FUSE is installed in the machine, since Geo-replication module
mounts the GlusterFS volume using FUSE to sync data.
* If the Slave is a volume, check if that volume is started.
* If the Slave is a plain directory, check if the directory has been
created already with the required permissions.
* If GlusterFS 3.2 is not installed in the default location (in Master)
and has been prefixed to be installed in a custom location, configure
the gluster-command for it to point to the exact location.
* If GlusterFS 3.2 is not installed in the default location (in slave)
and has been prefixed to be installed in a custom location, configure
the remote-gsyncd-command for it to point to the exact place where
gsyncd is located.

*Intermediate Master goes to Faulty State*

*Description:* In a cascading set-up, the intermediate master goes to
faulty state with the following log:

raise RuntimeError("aborting on uuid change from %s to %s" % \
RuntimeError: aborting on uuid change from
af07e07c-427f-4586-ab9f-4bf7d299be81 to
de6b5040-8f4e-4575-8831-c4f55bd41154

*Solution:* In a cascading set-up the Intermediate master is loyal to
the original primary master. The above log means that the GlusterFS
Geo-replication module has detected change in primary master. If this is
the desired behavior, delete the config option volume-id in the session
initiated from the intermediate master.

Gluster 3.2: Rotating Logs[ << Rotating Logs] |
link:Gluster 3.2 Filesystem Administration Guide[ Table of Contents]
|Gluster 3.2: Troubleshooting POSIX ACLs[ Troubleshooting POSIX ACLs>>]
