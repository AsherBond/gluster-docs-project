See Also:

* link:Arch/Change Logging Translator Design[Change Logging Translator
Design]
* link:Arch/Active-Active GeoRep Design[Active-Active GeoRep Design]
* link:Arch/GeoRep Prior Work#Current_GeoRep[Current GeoRep]
* link:Arch/GeoRep Prior Work#X-Sync_Prototype[X-Sync Prototype]
* link:Arch/BitRot Detection[Bitrot Detection]

[[]]
Design Objectives
-----------------

This document contains notes on the proposed design for Parallel GeoRep.
Parallel GepRep is intended to be a high performance the successor to
the current Gusterfs Master/Slave GeoRep. Design objectives include:

* Parallelize GeoReplication of individual volumes across all cluster
nodes
* Introduce more scalabable change detection mechanism, where
computational complexity of change detection better correlates to actual
changes in directory rather than total files in directory or overall
filesystem. Limitations of current approach:
** Current GeoRep relies on Merkel-tree based Marker Framework to prune
directory crawl. Pruning has been shown to be ineffective for certain
users workloads where file insertion is randomly distributed across
directory hierarchy (ie: a large percentage of directories contain
changes within GeoRep scan interval).
** Reduce cost of scanning directories that contain changes
*** For directories with large numbers of files, cost of directory scan
correlated with overall files in directory, rather than the small
percentage of files that may have actually changes. This means that scan
performance degrades with increasing number of files.
*** The STATing of individual files for timestamp during scan can be
particularly problematic due to excessive disk traffic caused by reading
of file inode on-disk.
* Provide foundation for Active-Active GeoRep, including latent support
for sparse replicas
* Provide re-useable change detection framework that may be used for
other Glusterfs services, such as Bit Rot detection, DeDupe ...

[[]]
Prior Glusterfs Work
--------------------

* link:Arch/GeoRep Prior Work#Current_GeoRep[Current GeoRep]
* link:Arch/GeoRep Prior Work#X-Sync_Prototype[X-Sync Prototype]
*
link:Arch/GeoRep Prior Work#Performance_Implications_of_Various_File_Copy_Mechanisms[Performance
Implications of Various File Copy Mechanisms]

[[]]
Proposed Approach
-----------------

The proposed approach builds on the X-Sync prototype, with the following
key enhancements:

* Includes support for Deletes. Specifically the inclusion of
'Tombstones' to identify changed files
* Includes log based change detection subsystem as primary mechanism to
identify changed files. X-Time and X-file continue to be used as back-up
under situations such as server crash where log data may be incomplete.

[[]]
Log Based Change Detection
--------------------------

see also : link:Arch/Change Logging Translator Design[Arch/Change
Logging Translator Design]

[[]]
Gluster operations requiring logging to support GeoRep
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Overall, there are four categories of operations that will need to be
monitored

* File content related operations
* File attribute related operations
* Directory related operations

[[]]
File content related operations
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

As a general rule, we would like to queue a replication operation for a
file upon file close (where the file has previously been opened for
write or append access). In this scenario, the close operation
implicitly creates a new version of the file.

* Question: How to we detect file close. Don't see close FOP in xlator.h

There are certain situations were we may need to propagate changes when
there is no corresponding close:

* For files such as log files where file is open for extended periods of
time and data is continuously appended at the end of the file. In this
case, we may want to define minimum frequency of updates (e.g.: every
min)
* For NFS operations, where there are no corresponding opens. One might
be able to infer close by having file stable for a given period of time.
** NFS operations may be identified via anonymous FD
** NFS accesses may be via GFID rather than path, so will need to map to
corresponding path before queueing for replication since GFIDs are
different for different clusters
* Upon recovery after system or glusterfsd crash.

Relevant FOPs:

* create
* open (when opened for write or append)
* truncate
* ftruncate
* writev - needed for log writes and NFS writes to determine recent
activity.

[[]]
File attribute related operations
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

File Extended Attribute related FOPS. Ignore operations to system.* and
gluster.*

* setxattr
* fsetxattr
* removexattr
* fremovexattr

*Question:* Do we need to also monitor file attributes

* setattr
* fsetattr

[[]]
Directory related operations
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Directory related FOPS:

* mkdir
* rmdir
* symlink
* rename
* link

[[]]
Error Recovery
~~~~~~~~~~~~~~

A potential approach would be to use the change log from another replica
to continue operating using the change log based operation. One could
revert back to the local server that the point that integrity has been
re-established with the local replica change log (ie: when replication
is past any gaps in the local change log).

* Question: Is there an easy way to access a brick change log remotely,
if needed

The backup strategy for any situation where the change log cannot be
used (graps in log, accidental deletion of log ...) is to use xfind to
identify all changes relative to the last trusted replication point
(last trustworty point in change log). One can fall back to change log
at the point that xfind has verified all changes up to a trusted point
in the new change log/

[[]]
Situations Requiring Special Handling
-------------------------------------

Instances where propagation of changes to an individual brick might been
delayed in time, such as changes arriving on a brick after more recent
changes might have been propagated to another node:

* Rebalance: file moves between bricks concurrent with replication
* Self-heal: older file created on another node appears on local as a
result of self-heal operation
** potential solution: disable replication if brick has files in outcast
state
* Files replicated from another node that will need to be propagated to
another node.
* rmdir: Since rmdir is a volume-wide operation spanning multiple
bricks, care must be taken since different brick replication processes
m,ay be in different phases of executing related operations, such as
deleting files within the directory prior to rmdir.
** Recommend that we treat rmdir a conventionalized operation handled by
each brick replication process
** rmdir only performed on remote slave node is salve directory is
empty, otherwise skip and let the rmdir from another brick handle the
removal
** care is needed because it is possible to perform create mkdir
immediately folloowing rmdir, followed by new file cdreation, and these
subsequent operations shouldn't be lost.

Handing of open files, log files, and NFS:

* As a general rule, would like to delay GeoReplication of a file while
undergoing a active modification
** By default, use close event to signify that a file is ready for
propagation
*** Issue: Xtime currently reflects time of last write rather than time
of closed
*** It is possible that replication of a file is delayed until after
files with a more recent timestamp (depending on interval between last
write and close.
**** Possible solution: Updated xtime on close.
**** Any special error handling on crash
* NFSv3 stateless. How do we close event?
** recommendation: Use lifecycle of anonymous fd. Consider
allocation/deallocation of fd as an open/close event
*** open-for-write is actually implied by the first write to an
anonymous fd
*** Gruster may be too aggressive in deallocating anonymous fd's today.
may want to extend timelot to a longer interval.
* log files: May want to identify a class of files that, once modified,
will always replicate changes after a delay threshold is exceeded.
** what is policy for identifying this class of files
** when should xtime be updated?

Overall issue: Proper handling of delayed update files

* proper handling of xtime
* proper identification of pending (delayed write) files after crash.
** one possible solution: Borrow concept from index. Every file where
georep is skipped due to delay/pending has corresponding link file
entered into a pending directory.

[[]]
Questions and Issues
--------------------

* How should we handle replication of log files and other files that are
open for extended periods of time
* How should we handle garbage collection for tombstones?

[[]]
Additional Issues and Considerations for A-A GeoRep
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

[[]]
Additional Issues for Sparse Replicas
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
