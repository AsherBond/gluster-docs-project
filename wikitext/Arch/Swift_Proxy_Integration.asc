[[]]
Object Storage Patterns of Use
------------------------------

While the gluster implementation of object storage layers above the core
Glusterfs, some distinct patterns of use are starting to appear that
differ from traditional direct consumption of a POSIX filesystem. Below
are a few characteristics of the UFO workload that are starting to
emerge:

* File names are often randomly generated UUID strings. Object metadata,
such as collections of files, versioning ... are often stored explicitly
in some NoSQL store (Mongodb, Cassandra) rather than via filesystem
hierarchy.
* Containers can have huge numbers of objects.
* Sub-container lifecycle is implicit rather than explicit
** Swift has not direct concept of a sub-container, sub-containers are
inferred by convention though the presence of "/" in a path.
** Sub-containers (and associated glusterfs sub-directories) are created
on-demand based on presence in a PUT request
*** Concurrent PUTs can lead to concurrent mkdir requests, with no
coordination on the host
** Sub-containers will need to be garbage collected when last object
removed from a sub-container
* The presence of a load balance from of a collection of proxy servers
means that multiple PUTs or GETs from the same client may fan-out to
multiple Proxy servers. Can create the perception of false sharing or
false concurrency.
* In the case of slow response times, REST API calls can time-out after
120sec (and be retried) whereas traditional POSIX operations block until
completion
** A time-out of a REST API call by the client can go undetected by the
server
** A client can retry a time-out request (potentially directed to a
different PROXY server), resulting in multiple active version of the
same request
** REST API requires difference mechanisms for back-pressure and
flow-control to avoid multiple retries
* Objects have discrete versions.
** With the exception metadata, objects are never modified, just
replaced
** This means that each version of an object has it's own unique GFID
and loca;fs inode value.

[[]]
Mapping of Swift Operations to Gluster Operations
-------------------------------------------------

Current mapping of Swift concepts into Gluster objects:

* IP Address: Cluster-side namespace (may include 1 or more volumes as
distinct entity)
* Swift Account: A Gluster volume
* Swift Container: Top level directory, owned by account
* Swift Object: File and associated sub-directory path
* Swift User: Some authorized user, can be used for ACL based access

[[]]
Object Operations
~~~~~~~~~~~~~~~~~

[width="100%",cols="<25%,<50%,<25%",options="header",]
|=======================================================================
|Operation |Gluster Mapping |Further optimizations |+Mapping of Swift
Object Operations to Glusterfs
|PUT |[multiblock cell omitted] |queueing, co-routines and async-io

|POST (metadata update only) |[multiblock cell omitted]

|HEAD |[multiblock cell omitted]

|GET |[multiblock cell omitted] |sendfile (depends on libgfapi)

|DELETE |[multiblock cell omitted] |Clean-up of dangling sub-directories

|COPY (between containers) |[multiblock cell omitted] |Consider use of
link instead of copy. Works since files are always over-written (new
inode number) rather than modified

|REPLICATE |tbd, not implemented |[multiblock cell omitted]
|=======================================================================

[[]]
ContainerOperations
~~~~~~~~~~~~~~~~~~~

[width="100%",cols="<25%,<50%,<25%",options="header",]
|=======================================================================
|Operation |Gluster Mapping |Further optimizations |+Mapping of Swift
Container Operations to Glusterfs
|PUT |[multiblock cell omitted] |[multiblock cell omitted]

|POST |[multiblock cell omitted] |[multiblock cell omitted]

|HEAD |[multiblock cell omitted] |[multiblock cell omitted]

|GET |[multiblock cell omitted] |[multiblock cell omitted]

|DELETE |[multiblock cell omitted] |[multiblock cell omitted]

|COPY (between containers) |tbd, not implemented |Issue: Need to
validate any latent support

|REPLICATE |tbd, not implemented |Issue: Need to validate any latent
support
|=======================================================================

[[]]
Account Operations
~~~~~~~~~~~~~~~~~~

[width="100%",cols="<25%,<50%,<25%",options="header",]
|=======================================================================
|Operation |Gluster Mapping |Further optimizations |+Mapping Swift
Account Operations to Glusterfs
|PUT |N/A |Currently treated as a volume lifecycle operatin

|POST |See container POST |Adds metadata to account

|HEAD |See container HEAD |today only return container count, not object
count

|GET |See container GET |today only return list of containers, no
internal meta-data

|DELETE |N/A |same issue as PUT

|REPLICATE |NA |May not exist in Swift
|=======================================================================

[[]]
Alternatives and Enhancements
-----------------------------

[[]]
Current Enhancements
~~~~~~~~~~~~~~~~~~~~

* AccurateSizeInListing option: Turns off gathering on/off gatering of
individual object size (still counts objects). Applies to both
Containers and Accounts (currently set of off in PDQ

[[]]
Proposed Enhancements
~~~~~~~~~~~~~~~~~~~~~

* Upgrade to Folsom
* Use of Marker Framework/Quota as source of sub-tree size and object
count (note: required enhancement to quota to supply object count)
* Automatic sub-directory deletion
** Sub-directories created by Swift implicitly by Swift as part of
object creation and marked by extended attribute.
** Upon file deletion, if subdirectory is empty and includes swift flag,
then safe to delete sub-directory.
* Adapt AIO to be co-routine friendly
** Potentially add multiple QoS queues within AIO code (C code and
helper threads)
* Optimized container GETs using opaque object as cursor - avoids having
to perform full enumeration and caching of directory sub-tree before
returning first 10k entries

[[]]
Potential Optimizations - Object Only Mode
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* Disallow sub-containers contained in path.
* Opaque directory hierarchy: Map of large containers (100L's to 1M's of
objects) into a set of hidden sub-directories with smaller directory
count.
* See also Mapping Alternative #1 below
** May want to store accounts in dedicated gluster volume of mapping to
multiple Gluster volumes within a single namespace
*** Requires use of symbolic links to make to volume at either an
Account or potentially container granularity

Proposed Alternative Mapping #1 (Object storage optimized)

* IP Address: Namespace (Gluster wide or potentially world-wide)
* Swift Account: Conceptually, "a guy with a credit card", maps to home
directory
* Swift Container: First level of sub-directories below home directory
* Swift Object: File and associated sub-directory path
* Note: Depending on implementation, accounts can all live on a single
sub-directory or span multiple sub-directories

==Potential Optimizations - Unified Mode

Proposed Alternative Mapping #2 (Unified File/Object optimized)

* IP Address: Namespace (Gluster wide or potentially world-wide)
* Swift Account: A user-specific container containing a set of exports
* Swift Container: A filesystem sub-tree that is exported on behalf of a
user.
** In this scenario, containers can be hierarchical
** Container is the full path starting with slash after account and
continuing until final slash before object
* Swift Object: Individual file. No path

[[]]
Objects to explore further
--------------------------

* How might we explore filters
** Rate Limiting
*** Potentially including Queue depth mgmt and QoS
** Path Mapping
* Optimization of container Gets and enumerate on-the-fly rather than
full sub-tree enumeration
** What are the rules for the ordering of the directory entries
** What are the rules for the cursor value, can we use to encode
gluster-specific information.
** Potential implementation
*** Depth first enumeration, stop after 10K values
*** Further enhancement: Individual bricks require 10K values in sorted
order, client performs merge. Client caches remaining returned values,
server caches remaining vlaues within most recently touched directory.

A potential alternative back-pressure scheme. Note: Depends on
non-blocking glusterfs IO.

* Co-routine issues non-blocking IO request to Glusterfs and yields
* Co-routine either wakes up on IO completion or when some threshold
timeout valid (ie: 90sec) is exceeded
** If IO operation completes first, success reported to client
** If time occurs first:
*** Pending/Retry response returned to client with unique etag value
*** Upon IO completion, response is stored with mcached with associated
etag value
*** Upon client retry, client performs lookup in mcache with etag first,
to gather pended value
**** Client may optionally perform head request to object to verify that
delayed response is still valid (GET/HEAD only).
