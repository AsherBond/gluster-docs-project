link:GlusterFest[Back to GlusterFest]

[[]]
Welcome
^^^^^^^

Thank you for helping to test the newest release of GlusterFS.

[[]]
Assemble test machines
^^^^^^^^^^^^^^^^^^^^^^

These tests use one, two, four, and optionally more servers; and one or
more clients. Use qemu/kvm or other virtualization products to create as
many guests as you need for the servers and clients. Of course you may
also use bare metal if you have it available. Setting up your servers
and clients is left as an exercise. Remember to enter the hostnames in
DNS and/or in /etc/hosts on all the nodes.

[[]]
Bugs, Bugzilla
^^^^^^^^^^^^^^

Any crashes, anomalies, please file a BZ at
https://bugzilla.redhat.com/enter_bug.cgi?product=GlusterFS

Note that you will need to create an account if you don't already have
one.

[[]]
Build Gluster
^^^^^^^^^^^^^

Get the source from
http://bits.gluster.com/pub/gluster/glusterfs/src/glusterfs-3.3.0beta4.tar.gz

Or get pre-built binaries from
http://bits.gluster.com/pub/gluster/glusterfs/3.3.0beta4/x86_64/

To build from source there are a few prerequisites (Fedora):

* automake
* autoconf
* libxml2-devel
* ncurses-devel
* readline-devel
* openssl-devel

In Ubuntu/Debian:

* automake
* autoconf
* libxml2-dev
* libncurses-dev
* libreadline-dev
* libssl-dev

You may also need these:

* libibverbs-devel
* libtool
* python-ctypes (Fedora)
* python-ctypeslib (Ubuntu/Debian)

In addition to the usual gcc, make, bison, and flex.

N.B. all the following commands should be run as root or with sudo.

Unpack the source, configure, and build: `
<br>% tar xzpf glusterfs-3.3.0beta4.tar.gz
<br>% cd glusterfs-3.3.0beta4
<br>% ./autogen.sh
<br>% ./configure
<br>% make
`

[[]]
Set up test machines
^^^^^^^^^^^^^^^^^^^^

To install glusterfs: `
<br>% make install
<br>&lt;repeat on each server and client&gt;
`

Or install the prebuilt binaries referenced above.

Create brick directories: `
<br>% mkdir /tmp/test{1,2,3,4,5}
<br>% chmod 01777 /tmp/test?
<br>&lt;repeat on each server&gt;
`

Start glusterd: `
<br>% /usr/local/sbin/glusterd
<br>&lt;repeat on each server&gt;
`

On each server use your prefered method to open ports 24007-24030 and
38465-38468 in your firewall. (Or completely disable your firewall.)

[[]]
Test 1 — Assemble a cluster:
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

 +
<On one server node (e.g. <node1name>) `
<br>% /usr/local/sbin/gluster peer status
<br>Number of Peers: 0
<br>% /usr/local/sbin/gluster peer probe &lt;node2name&gt;
<br>Probe successful
<br>% /usr/local/sbin/gluster peer probe &lt;node3name&gt;
<br>Probe successful
<br>% /usr/local/sbin/gluster peer probe &lt;node4name&gt;
<br>Probe successful
<br>% /usr/local/sbin/gluster peer status
<br>Number of Peers: 3
<br>
<br>Hostname: &lt;node2name&gt;
<br>Uuid: 5d25363f-1d09-44b5-94eb-258be64ec3f3
<br>State: Peer in Cluster (Connected)
<br>
<br>Hostname: &lt;node3name&gt;
<br>Uuid: 6e989fab-3126-43a2-9946-9bab2b90775d
<br>State: Peer in Cluster (Connected)
<br>
<br>Hostname: &lt;node4name&gt;
<br>Uuid: 0d4aedc5-239b-49b2-94e1-bb1eb9d21820
<br>State: Peer in Cluster (Connected)
`

Verify that each node has a unique UUID and all are Connected

[[]]
Test 2 — Create volumes:
^^^^^^^^^^^^^^^^^^^^^^^^

 +
<On one of the server nodes create a single brick volume> `
<br>% /usr/local/sbin/gluster volume create test1 &lt;node1name&gt;:/tmp/test1
<br>Creation of volume test1 has been successful. Please start the volume to access data.
` +
 +
<On one of the server nodes create a two brick distributed (DHT)
volume> +
% /usr/local/sbin/gluster volume create test2 <node1name>:/tmp/test2
<node2name>:/tmp/test2 +
Creation of volume test2 has been successful. Please start the volume to
access data.  +
 +
<On one of the server nodes create a two brick replicated (AFR)
volume> +
% /usr/local/sbin/gluster volume create test3 replica 2
<node1name>:/tmp/test3 <node2name>:/tmp/test3 +
Creation of volume test3 has been successful. Please start the volume to
access data.  +
 +
<On one of the server nodes create a four brick distributed, replicated
volume> +
% /usr/local/sbin/gluster volume create test4 replica 2
<node1name>:/tmp/test4 <node2name>:/tmp/test4 <node3name>:/tmp/test4
<node4name>:/tmp/test4 +
Creation of volume test4 has been successful. Please start the volume to
access data.  +
 +
<On one of the server nodes create a two brick striped volume> `
<br>% /usr/local/sbin/gluster volume create test5 stripe 2 &lt;node1name&gt;:/tmp/test5 &lt;node2name&gt;:/tmp/test5
<br>Creation of volume test5 has been successful. Please start the volume to access data.
<br>
<br>&lt;On one of the server nodes query volume info&gt;
<br>% gluster volume info
<br>
<br>Volume Name: test1
<br>Type: Distribute
<br>Status: Started
<br>Number of Bricks: 1
<br>Transport-type: tcp
<br>Bricks:
<br> Brick1: &lt;node1name&gt;:/tmp/test1
<br>
<br>Volume Name: test2
<br>Type: Distribute
<br>Status: Created
<br>Number of Bricks: 2
<br>Transport-type: tcp
<br>Bricks:
<br>Brick1: &lt;node1name&gt;:/tmp/test2
<br>Brick2: &lt;node2name&gt;:/tmp/test2
<br>
<br>Volume Name: test3
<br>Type: Replicate
<br>Status: Created
<br>Number of Bricks: 2
<br>Transport-type: tcp
<br>Bricks:
<br>Brick1: &lt;node1name&gt;:/tmp/test3
<br>Brick2: &lt;node2name&gt;:/tmp/test3
<br>
<br>Volume Name: test4
<br>Type: Distributed-Replicate
<br>Status: Created
<br>Number of Bricks: 2 x 2 = 4
<br>Transport-type: tcp
<br>Bricks:
<br>Brick1: &lt;node1name&gt;:/tmp/test4
<br>Brick2: &lt;node2name&gt;:/tmp/test4
<br>Brick1: &lt;node3name&gt;:/tmp/test4
<br>Brick2: &lt;node4name&gt;:/tmp/test4
<br>
<br>Volume Name: test5
<br>Type: Stripe
<br>Status: Created
<br>Number of Bricks: 2
<br>Transport-type: tcp
<br>Bricks:
<br>Brick1: &lt;node1name&gt;:/tmp/test5
<br>Brick2: &lt;node2name&gt;:/tmp/test5
`

Verify that each volume is correctly created.

[[]]
Test 3 — Start all volumes:
^^^^^^^^^^^^^^^^^^^^^^^^^^^

 +
<On one of the server nodes start the volumes> `
<br>% /usr/local/sbin/gluster volume start test1
<br>Starting volume test1 has been successful
<br>
<br>% /usr/local/sbin/gluster volume start test2
<br>Starting volume test2 has been successful
<br>
<br>% /usr/local/sbin/gluster volume start test3
<br>Starting volume test3 has been successful
<br>
<br>% /usr/local/sbin/gluster volume start test4
<br>Starting volume test4 has been successful
<br>
<br>% /usr/local/sbin/gluster volume start test5
<br>Starting volume test5 has been successful
<br>
<br>&lt;On one of the server nodes query volume info&gt;
<br>% gluster volume info
<br>
<br>
<br>Volume Name: test1
<br>Type: Distribute
<br>Status: Started
<br>Number of Bricks: 1
<br>Transport-type: tcp
<br>Bricks:
<br> Brick1: &lt;node1name&gt;:/tmp/test1
<br>
<br>Volume Name: test2
<br>Type: Distribute
<br>Status: Started
<br>Number of Bricks: 2
<br>Transport-type: tcp
<br>Bricks:
<br>Brick1: &lt;node1name&gt;:/tmp/test2
<br>Brick2: &lt;node2name&gt;:/tmp/test2
<br>
<br>Volume Name: test3
<br>Type: Replicate
<br>Status: Started
<br>Number of Bricks: 2
<br>Transport-type: tcp
<br>Bricks:
<br>Brick1: &lt;node1name&gt;:/tmp/test3
<br>Brick2: &lt;node2name&gt;:/tmp/test3
<br>
<br>Volume Name: test4
<br>Type: Distributed-Replicate
<br>Status: Started
<br>Number of Bricks: 2 x 2 = 4
<br>Transport-type: tcp
<br>Bricks:
<br>Brick1: &lt;node1name&gt;:/tmp/test4
<br>Brick2: &lt;node2name&gt;:/tmp/test4
<br>Brick1: &lt;node3name&gt;:/tmp/test4
<br>Brick2: &lt;node4name&gt;:/tmp/test4
<br>
<br>Volume Name: test5
<br>Type: Stripe
<br>Status: Started
<br>Number of Bricks: 2
<br>Transport-type: tcp
<br>Bricks:
<br>Brick1: &lt;node1name&gt;:/tmp/test5
<br>Brick2: &lt;node2name&gt;:/tmp/test5
`

Verify that each volume is correctly started.

[[]]
Test 4a — Mount all volumes on client(s) using native gluster:
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

 +
<On one or more of the client nodes mount the volumes> `
<br>% mount -t glusterfs &lt;node1name&gt;:test1 &lt;mountpoint1&gt;
<br>
<br>% mount -t glusterfs &lt;node1name&gt;:test2 &lt;mountpoint2&gt;
<br>
<br>% mount -t glusterfs &lt;node1name&gt;:test3 &lt;mountpoint3&gt;
<br>
<br>% mount -t glusterfs &lt;node1name&gt;:test4 &lt;mountpoint4&gt;
<br>
<br>% mount -t glusterfs &lt;node1name&gt;:test5 &lt;mountpoint5&gt;
`

Verify using mount or df that all volumes have been successfully mounted

[[]]
Test 4b — Mount all volumes on client(s) using NFS:
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

 +
<On one or more of the client nodes mount the volumes> `
<br>% mount &lt;node1name&gt;:/test1 &lt;mountpoint1&gt;
<br>
<br>% mount &lt;node1name&gt;:/test2 &lt;mountpoint2&gt;
<br>
<br>% mount &lt;node1name&gt;:/test3 &lt;mountpoint3&gt;
<br>
<br>% mount &lt;node1name&gt;:/test4 &lt;mountpoint4&gt;
<br>
<br>% mount &lt;node1name&gt;:/test5 &lt;mountpoint5&gt;
`

Verify using mount or df that all volumes have been successfully mounted

[[]]
Test 5a — Run I/O on client(s) over native gluster:
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Drive I/O to the volumes from the clients. E.g. this could be as simple
as copying a few files to and from the volume or as involved as running
your favorite benchmark. Try to exercise as many different forms of file
I/O as you can. Light load, heavy load, be creative.

[[]]
Test 5b — Run I/O on client(s) over NFS:
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

See test 5a.

[[]]
Test 6 — Stop all volumes:
^^^^^^^^^^^^^^^^^^^^^^^^^^

TBD

[[]]
Test 7 — Delete all volumes:
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

TBD

[[]]
Test 8 — Detach all peers:
^^^^^^^^^^^^^^^^^^^^^^^^^^

TBD
