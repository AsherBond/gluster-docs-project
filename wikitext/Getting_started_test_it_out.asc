[[]]
Testing your swanky new Gluster deploy
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Now that we have a volume created, what on earth are we supposed to do
with it? These last steps show how to actually start using the cluster.

First, let’s start the volume.

`gluster volume start gv0  && gluster volume info gv0`

You should now see that the Status has changed from “Created” to
“Started”. Next, let’s create a mount point to test with. To help you
understand the concept of how Gluster works, I recommend using a
different machine than your Gluster servers, but there is no reason it
won’t work from the servers if you choose to do so.

`mkdir -p /mnt/gluster/gv0`

The path of the mount point is, again, arbitrary. I added a /mnt/gluster
directory instead of simple choosing /mnt/gv0 to denote that this is a
Gluster mount, but if you want to save the extra typing, feel free. The
one mistake that seems to get made fairly often by new users is to have
the mount path be the same as the export path. For example, having the
bricks mounted at /export/brick1 and having the volume mount point be in
/export/gluster. There is nothing wrong with this from a purely
technical standpoint, it will work. But trying to keep that straight at
2 in the morning when your pager goes off might not be as easy as you
think.

When mounting Gluster, you have three different protocols possible at
your disposal. Keep in mind that for NFS and CIFS, Gluster does not have
“automatic failover” available without configuring additional software
such as CTDB or ucarp. If having failover available without any manual
intervention being required, consider using the GlusterFS native client.
Although it requires additional packages, they are small and easy to
maintain, and for most users will be less of a hassle that setting up
your own failover manually.

* GlusterFS native client – Requires the glusterfs and glusterfs-fuse
packages to be installed on the client you want to mount from

`mount -t glusterfs node01.yourdomain.net:/gv0 /mnt/gluster/gv0`

* NFS - Gluster uses its own implementation of an NFS server, based on
NFSv3. If you want to use NFS with Gluster, just make sure that you have
the default NFS services disabled on the servers. As you will see below,
there is no need to modify the /etc/exports file to access Gluster via
NFS, you simply specify the mount. To mount using NFS:

For distributions that use NFSv3 -

`mount -t nfs node01.yourdomain.net:/gv0 /mnt/gluster/gv0`

For distributions that use NFSv4 -

mount -t nfs -overs=3 node01.yourdomain.net:/gv0 /mnt/gluster/gv0

* CIFS/Samba

`mount -t cifs node01.yourdomain.net:/gv0 /mnt/gluster/gv0`

From there, you can test writing data into the cluster by copying files
directly onto the client mount point. If you mount the same access point
from another client, you will see that the files you copied in are
there. Removing files from one client will show them removed on the
other client (this can appear to take some time. In most cases, however,
a performance caching feature of Gluster simply needs to refresh before
it sees the changes. You can force this in most cases by running `ls -R`
against the client mount point.
